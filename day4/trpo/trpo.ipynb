{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZOLkEzexcvHm"
   },
   "source": [
    "# Trust Region Policy Optimization Practice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6nTDAymrIVwI"
   },
   "source": [
    "# -1. Setting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yXX4sZB3c0SP"
   },
   "source": [
    "If you run in jupyter, turn \n",
    "\n",
    "```\n",
    "colab = False\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 22886,
     "status": "ok",
     "timestamp": 1627529240145,
     "user": {
      "displayName": "Jaeuk Shin",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh2HvwZBvQE7zU0ynPZa6sKJxKuEKKAqVSaYkNwrg=s64",
      "userId": "00192274820623998851"
     },
     "user_tz": -540
    },
    "id": "HEtOZ-YkcwJG",
    "outputId": "81a7eab1-8175-43c3-f69a-198d2977549a"
   },
   "outputs": [],
   "source": [
    "colab = False\n",
    "if colab:\n",
    "    !pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
    "    !apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1\n",
    "    !apt-get update > /dev/null 2>&1\n",
    "    !apt-get install cmake > /dev/null 2>&1\n",
    "    !pip install --upgrade setuptools 2>&1\n",
    "    !pip install ez_setup > /dev/null 2>&1\n",
    "    !pip3 install box2d-py\n",
    "    !pip3 install gym[Box_2D]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 20933,
     "status": "ok",
     "timestamp": 1627529265445,
     "user": {
      "displayName": "Jaeuk Shin",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh2HvwZBvQE7zU0ynPZa6sKJxKuEKKAqVSaYkNwrg=s64",
      "userId": "00192274820623998851"
     },
     "user_tz": -540
    },
    "id": "3HQocIk1dO8E",
    "outputId": "baaa63a6-526b-4658-ddba-296346ce7247"
   },
   "outputs": [],
   "source": [
    "if colab:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "\n",
    "    %cd /content/drive/MyDrive/rl-master/rl-master/day4/trpo\n",
    "    !ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 352,
     "status": "ok",
     "timestamp": 1627529728996,
     "user": {
      "displayName": "Jaeuk Shin",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh2HvwZBvQE7zU0ynPZa6sKJxKuEKKAqVSaYkNwrg=s64",
      "userId": "00192274820623998851"
     },
     "user_tz": -540
    },
    "id": "7pnRhmQQcvHn"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import csv\n",
    "import torch\n",
    "import os\n",
    "import copy\n",
    "import gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Independent\n",
    "from torch.distributions.normal import Normal\n",
    "from torch.optim import Adam\n",
    "from memory import OnPolicyMemory\n",
    "from utils import *\n",
    "import glfw\n",
    "import mujoco_py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 465,
     "status": "ok",
     "timestamp": 1627529310282,
     "user": {
      "displayName": "Jaeuk Shin",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh2HvwZBvQE7zU0ynPZa6sKJxKuEKKAqVSaYkNwrg=s64",
      "userId": "00192274820623998851"
     },
     "user_tz": -540
    },
    "id": "O1ndOboccvHo",
    "outputId": "0f94d0c5-6a88-4b57-e27f-6e4977ced7cd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current device :  cuda\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('current device : ', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w0R0gU2pcvHp"
   },
   "source": [
    "# 0. Network Architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 361,
     "status": "ok",
     "timestamp": 1627529325380,
     "user": {
      "displayName": "Jaeuk Shin",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh2HvwZBvQE7zU0ynPZa6sKJxKuEKKAqVSaYkNwrg=s64",
      "userId": "00192274820623998851"
     },
     "user_tz": -540
    },
    "id": "NX1R4ZqNcvHq"
   },
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    def __init__(self, obs_dim, act_dim, hidden1, hidden2):\n",
    "        # actor f_\\phi(s)\n",
    "        super(Actor, self).__init__()\n",
    "        self.fc1 = nn.Linear(obs_dim, hidden1)\n",
    "        self.fc2 = nn.Linear(hidden1, hidden2)\n",
    "\n",
    "        self.fc3 = nn.Linear(hidden2, act_dim)  # for \\mu\n",
    "        self.fc4 = nn.Linear(hidden2, act_dim)  # for \\sigma\n",
    "\n",
    "    def forward(self, obs):\n",
    "        x = torch.tanh(self.fc1(obs))\n",
    "        x = torch.tanh(self.fc2(x))\n",
    "\n",
    "        mu = self.fc3(x)\n",
    "        log_sigma = self.fc4(x)\n",
    "\n",
    "        sigma = torch.exp(log_sigma)\n",
    "\n",
    "        return mu, sigma\n",
    "\n",
    "    def log_prob(self, obs, act):\n",
    "        mu, sigma = self.forward(obs)\n",
    "        act_distribution = Independent(Normal(mu, sigma), 1)\n",
    "        log_prob = act_distribution.log_prob(act)\n",
    "        return log_prob\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    # critic V(s ; \\theta)\n",
    "    def __init__(self, obs_dim, hidden1, hidden2):\n",
    "        super(Critic, self).__init__()\n",
    "        self.fc1 = nn.Linear(obs_dim, hidden1)\n",
    "        self.fc2 = nn.Linear(hidden1, hidden2)\n",
    "        self.fc3 = nn.Linear(hidden2, 1)\n",
    "\n",
    "    def forward(self, obs):\n",
    "        x = torch.tanh(self.fc1(obs))\n",
    "        x = torch.tanh(self.fc2(x))\n",
    "\n",
    "        return self.fc3(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O4h65Dy-cvHq"
   },
   "source": [
    "# 1. Agent Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 507,
     "status": "ok",
     "timestamp": 1627529334584,
     "user": {
      "displayName": "Jaeuk Shin",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh2HvwZBvQE7zU0ynPZa6sKJxKuEKKAqVSaYkNwrg=s64",
      "userId": "00192274820623998851"
     },
     "user_tz": -540
    },
    "id": "W-oN94SZcvHr"
   },
   "outputs": [],
   "source": [
    "class TRPOAgent:\n",
    "    def __init__(\n",
    "                 self,\n",
    "                 obs_dim,\n",
    "                 act_dim,\n",
    "                 hidden1=64,\n",
    "                 hidden2=32,\n",
    "                 ):\n",
    "\n",
    "        self.obs_dim = obs_dim\n",
    "        self.act_dim = act_dim\n",
    "        self.hidden1 = hidden1\n",
    "        self.hidden2 = hidden2\n",
    "\n",
    "        self.pi = Actor(obs_dim, act_dim, hidden1, hidden2).to(device)\n",
    "        self.V = Critic(obs_dim, hidden1, hidden2).to(device)\n",
    "\n",
    "    def act(self, obs, deterministic=False):\n",
    "        obs = torch.tensor(obs, dtype=torch.float).to(device)\n",
    "        with torch.no_grad():\n",
    "            mu, sigma = self.pi(obs)\n",
    "            if deterministic:\n",
    "                action = mu\n",
    "                log_prob = None\n",
    "                val = None\n",
    "            else:\n",
    "                act_distribution = Independent(Normal(mu, sigma), 1)\n",
    "                action = act_distribution.sample()\n",
    "                log_prob = act_distribution.log_prob(action)\n",
    "                val = self.V(obs)\n",
    "                log_prob = log_prob.cpu().numpy()\n",
    "                val = val.cpu().numpy()\n",
    "\n",
    "        action = action.cpu().numpy()\n",
    "        \n",
    "\n",
    "        return action, log_prob, val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xweZIpcicvHr"
   },
   "source": [
    "# 2. Policy & Value Function Approximation Update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-aJyKaVEyds8"
   },
   "source": [
    "Objective:\n",
    "\\begin{align*}\n",
    "g = \\nabla_\\phi J(\\phi) &\\approx \\nabla_\\phi \\mathbb{E}_{s \\sim \\rho_{\\phi_{\\text{old}}}, a \\sim \\pi_{\\phi_{\\text{old}}}}\\left( \\frac{\\pi_{\\phi}(s, a)}{\\pi_{\\phi_{\\text{old}}}(s, a)} A^{\\pi_{\\phi_{\\text{old}}}}(s, a) \\right) \\\\\n",
    "&\\approx \\nabla_\\phi \\frac{1}{N} \\sum_{i = 1}^N \\left( \\frac{\\pi_{\\phi}(s_i, a_i)}{\\pi_{\\phi_{\\text{old}}}(s_i, a_i)} \\hat A(s_i, a_i) \\right).\n",
    "\\end{align*} \\\\\n",
    "Since we take into account approximated trust region constraint, the final update direction is\n",
    "\\begin{equation*}\n",
    "s = H^{-1}g, \\quad H s = g,\n",
    "\\end{equation*}\n",
    " and the stepsize is\n",
    " \\begin{equation*}\n",
    "\\alpha = \\sqrt{\\frac{2\\delta}{g^\\top H^{-1} g}}.\n",
    " \\end{equation*}\n",
    " Thus, the update is done as follows:\n",
    " \\begin{gather*}\n",
    " \\phi_{\\text{old}} \\longleftarrow \\phi, \\\\\n",
    "\\phi \\longleftarrow \\phi + \\alpha \\cdot s.\n",
    " \\end{gather*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 363,
     "status": "ok",
     "timestamp": 1627529340454,
     "user": {
      "displayName": "Jaeuk Shin",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh2HvwZBvQE7zU0ynPZa6sKJxKuEKKAqVSaYkNwrg=s64",
      "userId": "00192274820623998851"
     },
     "user_tz": -540
    },
    "id": "nowAebz5cvHs"
   },
   "outputs": [],
   "source": [
    "def update(agent, memory, critic_optim, delta, num_updates):\n",
    "    \n",
    "    batch = memory.load()\n",
    "\n",
    "    states = torch.Tensor(batch['state']).to(device)\n",
    "    actions = torch.Tensor(batch['action']).to(device)\n",
    "    target_v = torch.Tensor(batch['val']).to(device)\n",
    "    A = torch.Tensor(batch['A']).to(device)\n",
    "    old_log_probs = torch.Tensor(batch['log_prob']).to(device)\n",
    "    \n",
    "    for _ in range(num_updates):\n",
    "        ################\n",
    "        # train critic #\n",
    "        ################\n",
    "        out = agent.V(states)\n",
    "        critic_loss = torch.mean((out - target_v)**2)\n",
    "\n",
    "        critic_optim.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        critic_optim.step()\n",
    "\n",
    "        ###################\n",
    "        # policy gradient #\n",
    "        ###################\n",
    "        log_probs = agent.pi.log_prob(states, actions)\n",
    "\n",
    "        # TODO : calculate below to get probabiltiy ratio\n",
    "        # Hint : use log_probs and old_log_probs\n",
    "        # \\pi(a_t | s_t ; \\phi) / \\pi(a_t | s_t ; \\phi_old)\n",
    "        #prob_ratio = torch.exp()\n",
    "        prob_ratio = torch.exp(log_probs - old_log_probs)\n",
    "\n",
    "        actor_loss = torch.mean(prob_ratio * A)\n",
    "        loss_grad = torch.autograd.grad(actor_loss, agent.pi.parameters())\n",
    "        # flatten gradients of params\n",
    "        g = torch.cat([grad.view(-1) for grad in loss_grad]).data\n",
    "\n",
    "        s = cg(fisher_vector_product, g, agent.pi, states)\n",
    "\n",
    "        sAs = torch.sum(fisher_vector_product(s, agent.pi, states) * s, dim=0, keepdim=True)\n",
    "        step_size = torch.sqrt(2 * delta / sAs)[0]    # stepsize : move as far as possible within trust region\n",
    "        step = step_size * s\n",
    "\n",
    "        old_actor = Actor(agent.obs_dim, agent.act_dim, agent.hidden1, agent.hidden2).to(device)\n",
    "        old_actor.load_state_dict(agent.pi.state_dict())\n",
    "\n",
    "        params = flat_params(agent.pi)\n",
    "\n",
    "        backtracking_line_search(old_actor, agent.pi, actor_loss, g,\n",
    "                                 old_log_probs, params, step, delta, A, states, actions)    # line search => for improvement guarantee!\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 338,
     "status": "ok",
     "timestamp": 1627529343001,
     "user": {
      "displayName": "Jaeuk Shin",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh2HvwZBvQE7zU0ynPZa6sKJxKuEKKAqVSaYkNwrg=s64",
      "userId": "00192274820623998851"
     },
     "user_tz": -540
    },
    "id": "EpuMGH3fcvHt"
   },
   "outputs": [],
   "source": [
    "def evaluate(agent, env, num_episodes=5):\n",
    "\n",
    "    scores = np.zeros(num_episodes)\n",
    "    for i in range(num_episodes):\n",
    "        obs = env.reset()\n",
    "        done = False\n",
    "        score = 0.\n",
    "        while not done:\n",
    "            # if i == 0:\n",
    "            #     env.render()\n",
    "            action = agent.act(obs, deterministic=True)[0]\n",
    "            obs, rew, done, _ = env.step(action)\n",
    "            score += rew\n",
    "        # if i == 0:\n",
    "        #     env.close()\n",
    "        #     glfw.terminate()\n",
    "            \n",
    "        scores[i] = score\n",
    "    avg_score = np.mean(scores)\n",
    "    std_score = np.std(scores)\n",
    "    \n",
    "    return avg_score, std_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bfzZ3SU-cvHt"
   },
   "source": [
    "# 3. Training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 350,
     "status": "ok",
     "timestamp": 1627529503616,
     "user": {
      "displayName": "Jaeuk Shin",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh2HvwZBvQE7zU0ynPZa6sKJxKuEKKAqVSaYkNwrg=s64",
      "userId": "00192274820623998851"
     },
     "user_tz": -540
    },
    "id": "jLzSmhDocvHu"
   },
   "outputs": [],
   "source": [
    "def train(env, agent, max_iter, gamma=0.99, lr=3e-4, lam=0.95, delta=1e-3, steps_per_epoch=10000, eval_interval=10000, snapshot_interval=10000):\n",
    "    \n",
    "    obs_dim = env.observation_space.shape[0]\n",
    "    act_dim = env.action_space.shape[0]\n",
    "    max_ep_len = env._max_episode_steps\n",
    "    memory = OnPolicyMemory(obs_dim, act_dim, gamma, lam, lim=steps_per_epoch)\n",
    "    test_env = copy.deepcopy(env)\n",
    "    critic_optim = Adam(agent.V.parameters(), lr=lr)\n",
    "\n",
    "    save_path = './snapshots/'\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    os.makedirs('./learning_curves/', exist_ok=True)\n",
    "    log_file = open('./learning_curves/res.csv',\n",
    "                    'w',\n",
    "                    encoding='utf-8',\n",
    "                    newline=''\n",
    "                   )\n",
    "    logger = csv.writer(log_file)\n",
    "    num_epochs = max_iter // steps_per_epoch\n",
    "    total_t = 0\n",
    "    begin = time.time()\n",
    "    for epoch in range(num_epochs):\n",
    "        # start agent-env interaction\n",
    "        state = env.reset()\n",
    "        step_count = 0\n",
    "        ep_reward = 0\n",
    "\n",
    "        for t in range(steps_per_epoch):\n",
    "            # collect transition samples by executing the policy\n",
    "            action, log_prob, v = agent.act(state)\n",
    "\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            memory.append(state, action, reward, v, log_prob)\n",
    "\n",
    "            ep_reward += reward\n",
    "            step_count += 1\n",
    "\n",
    "            if (step_count == max_ep_len) or (t == steps_per_epoch - 1):\n",
    "                # termination of env by env wrapper, or by truncation due to memory size\n",
    "                s_last = torch.tensor(next_state, dtype=torch.float).to(device)\n",
    "                v_last = agent.V(s_last).item()\n",
    "                memory.compute_values(v_last)\n",
    "            elif done:\n",
    "                # episode done as the agent reach a terminal state\n",
    "                v_last = 0.0\n",
    "                memory.compute_values(v_last)\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "            if done:\n",
    "                state = env.reset()\n",
    "                step_count = 0\n",
    "                ep_reward = 0\n",
    "\n",
    "            if total_t % eval_interval == 0:\n",
    "                avg_score, std_score = evaluate(agent, test_env, num_episodes=5)\n",
    "                elapsed_t = time.time() - begin\n",
    "                print('[elapsed time : {:.1f}s| iter {}] score = {:.2f}'.format(elapsed_t, total_t, avg_score), u'\\u00B1', '{:.4f}'.format(std_score))\n",
    "                evaluation_log = [t, avg_score, std_score]\n",
    "                logger.writerow(evaluation_log)\n",
    "\n",
    "\n",
    "            if total_t % snapshot_interval == 0:\n",
    "                snapshot_path = save_path + 'iter{}_'.format(total_t)\n",
    "                # save weight & training progress\n",
    "                save_snapshot(agent, snapshot_path)\n",
    "\n",
    "            total_t += 1\n",
    "\n",
    "        # train agent at the end of each epoch\n",
    "        update(agent, memory, critic_optim, delta, num_updates=1)\n",
    "    log_file.close()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 367,
     "status": "ok",
     "timestamp": 1627529506725,
     "user": {
      "displayName": "Jaeuk Shin",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh2HvwZBvQE7zU0ynPZa6sKJxKuEKKAqVSaYkNwrg=s64",
      "userId": "00192274820623998851"
     },
     "user_tz": -540
    },
    "id": "mkQr9w8lcvHu",
    "outputId": "83290260-2ea5-4b53-a7d6-5d902fc475f5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "observation space dim. : 17 / action space dim. : 6\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('HalfCheetah-v2')\n",
    "obs_dim = env.observation_space.shape[0]\n",
    "act_dim = env.action_space.shape[0]\n",
    "print('observation space dim. : {} / action space dim. : {}'.format(obs_dim, act_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 378,
     "status": "ok",
     "timestamp": 1627530106463,
     "user": {
      "displayName": "Jaeuk Shin",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh2HvwZBvQE7zU0ynPZa6sKJxKuEKKAqVSaYkNwrg=s64",
      "userId": "00192274820623998851"
     },
     "user_tz": -540
    },
    "id": "aS6XCK63cvHv"
   },
   "outputs": [],
   "source": [
    "agent = TRPOAgent(obs_dim, act_dim, hidden1=128, hidden2=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1627530108621,
     "user": {
      "displayName": "Jaeuk Shin",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh2HvwZBvQE7zU0ynPZa6sKJxKuEKKAqVSaYkNwrg=s64",
      "userId": "00192274820623998851"
     },
     "user_tz": -540
    },
    "id": "ubINos42Bqt6",
    "outputId": "2aaef20b-0ff1-4be0-d467-781a03d436ab"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(agent.pi.parameters()).is_cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 182400,
     "status": "ok",
     "timestamp": 1627531048047,
     "user": {
      "displayName": "Jaeuk Shin",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh2HvwZBvQE7zU0ynPZa6sKJxKuEKKAqVSaYkNwrg=s64",
      "userId": "00192274820623998851"
     },
     "user_tz": -540
    },
    "id": "9K0q2WTDcvHv",
    "outputId": "d8371def-2860-4b54-ba6f-840681d7c13f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[elapsed time : 1.7s| iter 0] score = -48.80 ± 0.8013\n",
      "[elapsed time : 10.1s| iter 10000] score = -48.56 ± 1.1181\n",
      "[elapsed time : 18.3s| iter 20000] score = -42.14 ± 0.7771\n",
      "[elapsed time : 26.5s| iter 30000] score = -40.30 ± 0.6180\n",
      "[elapsed time : 35.1s| iter 40000] score = -44.53 ± 0.5490\n",
      "[elapsed time : 43.5s| iter 50000] score = -38.59 ± 1.0267\n",
      "[elapsed time : 52.2s| iter 60000] score = -45.89 ± 0.9653\n",
      "[elapsed time : 60.7s| iter 70000] score = -41.22 ± 0.9451\n",
      "[elapsed time : 69.0s| iter 80000] score = -44.23 ± 0.6897\n",
      "[elapsed time : 77.2s| iter 90000] score = -35.01 ± 0.4106\n",
      "[elapsed time : 85.8s| iter 100000] score = -38.00 ± 0.2626\n",
      "[elapsed time : 94.2s| iter 110000] score = -30.36 ± 0.5363\n",
      "[elapsed time : 102.7s| iter 120000] score = -35.58 ± 0.3909\n",
      "[elapsed time : 111.4s| iter 130000] score = -32.90 ± 1.5522\n",
      "[elapsed time : 119.9s| iter 140000] score = -28.92 ± 0.5863\n",
      "[elapsed time : 128.5s| iter 150000] score = -17.67 ± 0.9945\n",
      "[elapsed time : 137.2s| iter 160000] score = -12.77 ± 1.1046\n",
      "[elapsed time : 145.4s| iter 170000] score = -10.00 ± 0.7094\n",
      "[elapsed time : 153.8s| iter 180000] score = -3.51 ± 3.1146\n",
      "[elapsed time : 162.1s| iter 190000] score = -0.23 ± 1.7094\n",
      "[elapsed time : 170.3s| iter 200000] score = 3.68 ± 1.3862\n",
      "[elapsed time : 178.6s| iter 210000] score = 13.81 ± 1.1204\n",
      "[elapsed time : 187.3s| iter 220000] score = 0.49 ± 1.1799\n",
      "[elapsed time : 195.6s| iter 230000] score = 4.94 ± 1.3764\n",
      "[elapsed time : 203.7s| iter 240000] score = 9.46 ± 2.7992\n",
      "[elapsed time : 211.9s| iter 250000] score = 1.56 ± 1.2075\n",
      "[elapsed time : 220.4s| iter 260000] score = 19.16 ± 3.8633\n",
      "[elapsed time : 228.8s| iter 270000] score = 3.81 ± 1.2531\n",
      "[elapsed time : 237.4s| iter 280000] score = 64.39 ± 1.2609\n",
      "[elapsed time : 245.9s| iter 290000] score = 26.36 ± 1.4142\n",
      "[elapsed time : 254.1s| iter 300000] score = 130.38 ± 2.8311\n",
      "[elapsed time : 262.2s| iter 310000] score = 121.02 ± 16.0325\n",
      "[elapsed time : 270.4s| iter 320000] score = 125.08 ± 1.8549\n",
      "[elapsed time : 278.5s| iter 330000] score = 133.87 ± 2.5400\n",
      "[elapsed time : 286.7s| iter 340000] score = 84.92 ± 45.5501\n",
      "[elapsed time : 294.8s| iter 350000] score = 115.33 ± 2.6023\n",
      "[elapsed time : 303.1s| iter 360000] score = 141.98 ± 3.4409\n",
      "[elapsed time : 311.4s| iter 370000] score = 137.96 ± 2.1897\n",
      "[elapsed time : 319.9s| iter 380000] score = 193.46 ± 169.1102\n",
      "[elapsed time : 328.3s| iter 390000] score = 94.64 ± 30.3655\n"
     ]
    }
   ],
   "source": [
    "train(env, agent, max_iter=400000, gamma=0.99, lr=3e-4, lam=0.95, steps_per_epoch=10000, eval_interval=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hZBv4n0dcvHv"
   },
   "source": [
    "# 4. Watch how your agent solve the task!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "executionInfo": {
     "elapsed": 1070,
     "status": "ok",
     "timestamp": 1627529373727,
     "user": {
      "displayName": "Jaeuk Shin",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh2HvwZBvQE7zU0ynPZa6sKJxKuEKKAqVSaYkNwrg=s64",
      "userId": "00192274820623998851"
     },
     "user_tz": -540
    },
    "id": "DxeYWf1EcvHw"
   },
   "outputs": [],
   "source": [
    "if colab:\n",
    "    import gym\n",
    "    from gym.wrappers import Monitor\n",
    "    import glob\n",
    "    import io\n",
    "    import base64\n",
    "    from IPython.display import HTML\n",
    "    from pyvirtualdisplay import Display\n",
    "    from IPython import display as ipythondisplay\n",
    "\n",
    "    display = Display(visible=0, size=(1400, 900))\n",
    "    display.start()\n",
    "\n",
    "    def show_video():\n",
    "      mp4list = glob.glob('video/*.mp4')\n",
    "      if len(mp4list) > 0:\n",
    "        mp4 = mp4list[0]\n",
    "        video = io.open(mp4, 'r+b').read()\n",
    "        encoded = base64.b64encode(video)\n",
    "        ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n",
    "                    loop controls style=\"height: 400px;\">\n",
    "                    <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
    "                </video>'''.format(encoded.decode('ascii'))))\n",
    "      else: \n",
    "        print(\"Could not find video\")\n",
    "        \n",
    "\n",
    "    def wrap_env(env):\n",
    "      env = Monitor(env, './video', force=True)\n",
    "      return env\n",
    "\n",
    "    env = wrap_env(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 457
    },
    "executionInfo": {
     "elapsed": 11352,
     "status": "ok",
     "timestamp": 1627529448582,
     "user": {
      "displayName": "Jaeuk Shin",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh2HvwZBvQE7zU0ynPZa6sKJxKuEKKAqVSaYkNwrg=s64",
      "userId": "00192274820623998851"
     },
     "user_tz": -540
    },
    "id": "OoEoj-R-cvHw",
    "outputId": "6e537e5f-4530-4b3d-958d-789e781f50e4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading pre-trained weight...\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './snapshots/trained.pth.tar'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-9f335387c198>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwrap_env\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'./snapshots/trained.pth.tar'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/jaeuk/Samsung_RL/2021/day4/trpo/utils.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(agent, path, device)\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'loading pre-trained weight...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 148\u001b[0;31m     \u001b[0mcheckpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    149\u001b[0m     \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'pi'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m     \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mV\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'V'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/drl/lib/python3.6/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    569\u001b[0m         \u001b[0mpickle_load_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'encoding'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 571\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    572\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    573\u001b[0m             \u001b[0;31m# The zipfile reader is going to advance the current file position.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/drl/lib/python3.6/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    227\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'w'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/drl/lib/python3.6/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 210\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_open_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './snapshots/trained.pth.tar'"
     ]
    }
   ],
   "source": [
    "env = gym.make('LunarLanderContinuous-v2')\n",
    "if colab:\n",
    "  env = wrap_env(env)\n",
    "\n",
    "load_model(agent, './snapshots/trained.pth.tar', device)\n",
    "\n",
    "obs = env.reset()\n",
    "\n",
    "done = False\n",
    "score = 0.\n",
    "while not done:\n",
    "    env.render()\n",
    "    obs, rew, done, _ = env.step(agent.act(obs, deterministic=True)[0])\n",
    "    score += rew\n",
    "env.close()\n",
    "print('score : ', score)\n",
    "\n",
    "if colab:\n",
    "  show_video()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tiZ1VUHUHtfd"
   },
   "source": [
    "# Proximal Policy Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-U63srv3xcZE"
   },
   "source": [
    "In contrast to TRPO, PPO uses the following simple $1^{\\text{st}}$-order objective!\n",
    "\\begin{equation*}\n",
    "L(\\phi) \\approx \\frac{1}{N} \\sum_{i = 1}^N \\min\\left( r_i(\\phi)\\hat A_i, \\text{clip}(r_i(\\phi), 1 - \\varepsilon, 1 + \\varepsilon) \\hat A_i  \\right).\n",
    "\\end{equation*}\n",
    "While we performed complex parameter updates in TRPO, we just build the above loss and use popular optimizers provided by PyTorch..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XXmOA2yWHs2l"
   },
   "outputs": [],
   "source": [
    "from ppo import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6OCKvpG4RSS3"
   },
   "outputs": [],
   "source": [
    "env = gym.make('LunarLanderContinuous-v2')\n",
    "obs_dim = env.observation_space.shape[0]\n",
    "act_dim = env.action_space.shape[0]\n",
    "print('observation space dim. : {} / action space dim. : {}'.format(obs_dim, act_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GeUrio16HxSE"
   },
   "outputs": [],
   "source": [
    "ppo_agent = PPOAgent(obs_dim, act_dim, hidden1=128, hidden2=128, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3WQ8D5BiNmfd"
   },
   "outputs": [],
   "source": [
    "ppo_train(env, ppo_agent, max_iter=500000, gamma=0.99, lr=3e-4, lam=0.95, epsilon=0.2, steps_per_epoch=10000, eval_interval=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CHkH3BbhResG"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "trpo.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
