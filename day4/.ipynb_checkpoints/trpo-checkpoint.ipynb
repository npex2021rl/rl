{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trust Region Policy Optimization Practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import csv\n",
    "import torch\n",
    "import os\n",
    "import copy\n",
    "import gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Independent\n",
    "from torch.distributions.normal import Normal\n",
    "from torch.optim import Adam\n",
    "from itertools import chain\n",
    "from memory import OnPolicyMemory\n",
    "from utils import cg, fisher_vector_product, backtracking_line_search, update_model, flat_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current device :  cpu\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('current device : ', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Network Architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    def __init__(self, obs_dim, act_dim, hidden1, hidden2):\n",
    "        # actor f_\\phi(s)\n",
    "        super(Actor, self).__init__()\n",
    "        self.fc1 = nn.Linear(obs_dim, hidden1)\n",
    "        self.fc2 = nn.Linear(hidden1, hidden2)\n",
    "\n",
    "        self.fc3 = nn.Linear(hidden2, act_dim)  # for \\mu\n",
    "        self.fc4 = nn.Linear(hidden2, act_dim)  # for \\sigma\n",
    "\n",
    "    def forward(self, obs):\n",
    "        x = torch.tanh(self.fc1(obs))\n",
    "        x = torch.tanh(self.fc2(x))\n",
    "\n",
    "        mu = self.fc3(x)\n",
    "        log_sigma = self.fc4(x)\n",
    "\n",
    "        sigma = torch.exp(log_sigma)\n",
    "\n",
    "        return mu, sigma\n",
    "\n",
    "    def log_prob(self, obs, act):\n",
    "        mu, sigma = self.forward(obs)\n",
    "        act_distribution = Independent(Normal(mu, sigma), 1)\n",
    "        log_prob = act_distribution.log_prob(act)\n",
    "        return log_prob\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    # critic V(s ; \\theta)\n",
    "    def __init__(self, obs_dim, hidden1, hidden2):\n",
    "        super(Critic, self).__init__()\n",
    "        self.fc1 = nn.Linear(obs_dim, hidden1)\n",
    "        self.fc2 = nn.Linear(hidden1, hidden2)\n",
    "        self.fc3 = nn.Linear(hidden2, 1)\n",
    "\n",
    "    def forward(self, obs):\n",
    "        x = torch.tanh(self.fc1(obs))\n",
    "        x = torch.tanh(self.fc2(x))\n",
    "\n",
    "        return self.fc3(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Agent Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TRPOAgent:\n",
    "    def __init__(\n",
    "                 self,\n",
    "                 obs_dim,\n",
    "                 act_dim,\n",
    "\n",
    "                 hidden1=64,\n",
    "                 hidden2=32,\n",
    "                 ):\n",
    "\n",
    "        self.obs_dim = obs_dim\n",
    "        self.act_dim = act_dim\n",
    "        \n",
    "        self.hidden1 = hidden1\n",
    "        self.hidden2 = hidden2\n",
    "\n",
    "        self.pi = Actor(obs_dim, act_dim, hidden1, hidden2).to(device)\n",
    "        self.V = Critic(obs_dim, hidden1, hidden2).to(device)\n",
    "\n",
    "    def act(self, obs):\n",
    "        obs = torch.tensor(obs, dtype=torch.float).to(device)\n",
    "        with torch.no_grad():\n",
    "            mu, sigma = self.pi(obs)\n",
    "            act_distribution = Independent(Normal(mu, sigma), 1)\n",
    "            action = act_distribution.sample()\n",
    "\n",
    "            log_prob = act_distribution.log_prob(action)\n",
    "            val = self.V(obs)\n",
    "\n",
    "        action = action.cpu().numpy()\n",
    "        log_prob = log_prob.cpu().numpy()\n",
    "        val = val.cpu().numpy()\n",
    "\n",
    "        return action, log_prob, val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Actor & Critic Update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update(agent, memory, critic_optim, delta, num_updates):\n",
    "    \n",
    "    batch = memory.load()\n",
    "\n",
    "    states = torch.Tensor(batch['state']).to(device)\n",
    "    actions = torch.Tensor(batch['action']).to(device)\n",
    "    target_v = torch.Tensor(batch['val']).to(device)\n",
    "    A = torch.Tensor(batch['A']).to(device)\n",
    "    old_log_probs = torch.Tensor(batch['log_prob']).to(device)\n",
    "    \n",
    "    for _ in range(num_updates):\n",
    "        ################\n",
    "        # train critic #\n",
    "        ################\n",
    "        out = agent.V(states)\n",
    "        critic_loss = torch.mean((out - target_v)**2)\n",
    "\n",
    "        critic_optim.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        critic_optim.step()\n",
    "\n",
    "        ###############\n",
    "        # train actor #\n",
    "        ###############\n",
    "        log_probs = agent.pi.log_prob(states, actions)\n",
    "\n",
    "        # \\pi(a_t | s_t ; \\phi) / \\pi(a_t | s_t ; \\phi_old)\n",
    "        prob_ratio = torch.exp(log_probs - old_log_probs)\n",
    "\n",
    "        actor_loss = torch.mean(prob_ratio * A)\n",
    "        loss_grad = torch.autograd.grad(actor_loss, agent.pi.parameters())\n",
    "        # flatten gradients of params\n",
    "        g = torch.cat([grad.view(-1) for grad in loss_grad]).data\n",
    "\n",
    "        s = cg(fisher_vector_product, g, agent.pi, states)\n",
    "\n",
    "        sAs = torch.sum(fisher_vector_product(s, agent.pi, states) * s, dim=0, keepdim=True)\n",
    "        step_size = torch.sqrt(2 * delta / sAs)[0]\n",
    "        step = step_size * s\n",
    "\n",
    "        old_actor = Actor(agent.obs_dim, agent.act_dim, agent.hidden1, agent.hidden2)\n",
    "        old_actor.load_state_dict(agent.pi.state_dict())\n",
    "\n",
    "        params = flat_params(agent.pi)\n",
    "\n",
    "        backtracking_line_search(old_actor, agent.pi, actor_loss, g,\n",
    "                                 old_log_probs, params, step, delta, A, states, actions)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(agent, env, num_episodes=5):\n",
    "\n",
    "    scores = np.zeros(num_episodes)\n",
    "    for i in range(num_episodes):\n",
    "        obs = env.reset()\n",
    "        done = False\n",
    "        score = 0.\n",
    "        while not done:\n",
    "            action = agent.act(obs)[0]\n",
    "            obs, rew, done, _ = env.step(action)\n",
    "            score += rew\n",
    "\n",
    "        scores[i] = score\n",
    "    avg_score = np.mean(scores)\n",
    "    std_score = np.std(scores)\n",
    "    return avg_score, std_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(env, agent, max_iter, gamma=0.99, lr=3e-4, lam=0.95, delta=1e-3, steps_per_epoch=4000, eval_interval=4000):\n",
    "    \n",
    "    obs_dim = env.observation_space.shape[0]\n",
    "    act_dim = env.action_space.shape[0]\n",
    "    max_ep_len = env._max_episode_steps\n",
    "    memory = OnPolicyMemory(obs_dim, act_dim, gamma, lam, lim=steps_per_epoch)\n",
    "    test_env = copy.deepcopy(env)\n",
    "    critic_optim = Adam(agent.V.parameters(), lr=lr)\n",
    "    os.makedirs('./learning_curves/', exist_ok=True)\n",
    "    log_file = open('./learning_curves/res.csv',\n",
    "                    'w',\n",
    "                    encoding='utf-8',\n",
    "                    newline=''\n",
    "                   )\n",
    "    logger = csv.writer(log_file)\n",
    "    num_epochs = max_iter // steps_per_epoch\n",
    "    total_t = 0\n",
    "    begin = time.time()\n",
    "    for epoch in range(num_epochs):\n",
    "        # start agent-env interaction\n",
    "        state = env.reset()\n",
    "        step_count = 0\n",
    "        ep_reward = 0\n",
    "\n",
    "        for t in range(steps_per_epoch):\n",
    "            # collect transition samples by executing the policy\n",
    "            action, log_prob, v = agent.act(state)\n",
    "\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            memory.append(state, action, reward, v, log_prob)\n",
    "\n",
    "            ep_reward += reward\n",
    "            step_count += 1\n",
    "\n",
    "            if (step_count == max_ep_len) or (t == steps_per_epoch - 1):\n",
    "                # termination of env by env wrapper, or by truncation due to memory size\n",
    "                s_last = torch.tensor(next_state, dtype=torch.float).to(device)\n",
    "                v_last = agent.V(s_last).item()\n",
    "                memory.compute_values(v_last)\n",
    "            elif done:\n",
    "                # episode done as the agent reach a terminal state\n",
    "                v_last = 0.0\n",
    "                memory.compute_values(v_last)\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "            if done:\n",
    "                state = env.reset()\n",
    "                step_count = 0\n",
    "                ep_reward = 0\n",
    "\n",
    "            if total_t % eval_interval == 0:\n",
    "                avg_score, std_score = evaluate(agent, test_env, num_episodes=5)\n",
    "                elapsed_t = time.time() - begin\n",
    "                print('[elapsed time : {:.1f}s| iter {}] score = {:.2f}'.format(elapsed_t, total_t, avg_score), u'\\u00B1', '{:.4f}'.format(std_score))\n",
    "                evaluation_log = [t, avg_score, std_score]\n",
    "                logger.writerow(evaluation_log)\n",
    "\n",
    "            total_t += 1\n",
    "\n",
    "        # train agent at the end of each epoch\n",
    "        update(agent, memory, critic_optim, delta, num_updates=1)\n",
    "    log_file.close()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "observation space dim. : 8 / action space dim. : 2\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('LunarLanderContinuous-v2')\n",
    "obs_dim = env.observation_space.shape[0]\n",
    "act_dim = env.action_space.shape[0]\n",
    "print('observation space dim. : {} / action space dim. : {}'.format(obs_dim, act_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = TRPOAgent(obs_dim, act_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[elapsed time : 0.3s| iter 0] score = -218.82 ± 157.0771\n",
      "[elapsed time : 3.4s| iter 4000] score = -272.99 ± 177.5552\n",
      "[elapsed time : 6.6s| iter 8000] score = -358.84 ± 43.0449\n",
      "[elapsed time : 9.6s| iter 12000] score = -394.81 ± 162.3906\n",
      "[elapsed time : 12.5s| iter 16000] score = -257.32 ± 177.0494\n",
      "[elapsed time : 15.7s| iter 20000] score = -262.12 ± 160.9821\n",
      "[elapsed time : 18.9s| iter 24000] score = -209.06 ± 58.4583\n",
      "[elapsed time : 22.0s| iter 28000] score = -246.32 ± 136.6173\n",
      "[elapsed time : 25.1s| iter 32000] score = -219.50 ± 74.6381\n",
      "[elapsed time : 28.1s| iter 36000] score = -226.45 ± 113.7727\n",
      "[elapsed time : 32.4s| iter 40000] score = -122.09 ± 116.1492\n",
      "[elapsed time : 35.6s| iter 44000] score = -216.59 ± 129.6992\n",
      "[elapsed time : 38.8s| iter 48000] score = -205.48 ± 158.1739\n",
      "[elapsed time : 42.0s| iter 52000] score = -121.50 ± 71.5140\n",
      "[elapsed time : 45.4s| iter 56000] score = -226.53 ± 98.5263\n",
      "[elapsed time : 48.3s| iter 60000] score = -87.40 ± 22.1071\n",
      "[elapsed time : 51.4s| iter 64000] score = -201.65 ± 138.4642\n",
      "[elapsed time : 54.6s| iter 68000] score = -137.58 ± 123.5274\n",
      "[elapsed time : 57.8s| iter 72000] score = -87.00 ± 39.0707\n",
      "[elapsed time : 61.1s| iter 76000] score = -155.76 ± 41.9128\n",
      "[elapsed time : 64.2s| iter 80000] score = -171.37 ± 57.9763\n",
      "[elapsed time : 67.2s| iter 84000] score = -106.37 ± 83.1660\n",
      "[elapsed time : 70.3s| iter 88000] score = -122.09 ± 43.8207\n",
      "[elapsed time : 73.5s| iter 92000] score = -180.33 ± 106.2840\n",
      "[elapsed time : 76.6s| iter 96000] score = -106.07 ± 48.2788\n",
      "[elapsed time : 79.8s| iter 100000] score = -118.56 ± 44.1814\n",
      "[elapsed time : 82.9s| iter 104000] score = -77.14 ± 55.7032\n",
      "[elapsed time : 86.1s| iter 108000] score = -113.65 ± 47.2450\n",
      "[elapsed time : 89.1s| iter 112000] score = -63.99 ± 13.1264\n",
      "[elapsed time : 92.4s| iter 116000] score = -115.24 ± 57.5564\n",
      "[elapsed time : 96.0s| iter 120000] score = -100.61 ± 64.6893\n",
      "[elapsed time : 99.2s| iter 124000] score = -104.66 ± 59.7441\n",
      "[elapsed time : 102.4s| iter 128000] score = -77.47 ± 38.4840\n",
      "[elapsed time : 105.7s| iter 132000] score = -55.28 ± 42.3935\n",
      "[elapsed time : 109.1s| iter 136000] score = -97.92 ± 122.0468\n",
      "[elapsed time : 113.6s| iter 140000] score = -105.45 ± 57.2515\n",
      "[elapsed time : 116.8s| iter 144000] score = -41.48 ± 11.5139\n",
      "[elapsed time : 120.0s| iter 148000] score = -61.20 ± 19.6118\n",
      "[elapsed time : 123.2s| iter 152000] score = -64.61 ± 47.4620\n",
      "[elapsed time : 126.5s| iter 156000] score = -23.43 ± 29.6527\n",
      "[elapsed time : 129.7s| iter 160000] score = -38.20 ± 22.4999\n",
      "[elapsed time : 133.0s| iter 164000] score = -54.18 ± 22.4221\n",
      "[elapsed time : 137.1s| iter 168000] score = -58.80 ± 13.9097\n",
      "[elapsed time : 142.0s| iter 172000] score = -90.65 ± 76.4300\n",
      "[elapsed time : 147.7s| iter 176000] score = -8.25 ± 68.2458\n",
      "[elapsed time : 151.3s| iter 180000] score = -65.00 ± 54.8342\n",
      "[elapsed time : 154.8s| iter 184000] score = -69.58 ± 51.1874\n",
      "[elapsed time : 158.2s| iter 188000] score = -68.47 ± 54.9017\n",
      "[elapsed time : 161.5s| iter 192000] score = -50.95 ± 29.4392\n",
      "[elapsed time : 164.9s| iter 196000] score = -25.46 ± 12.8751\n",
      "[elapsed time : 169.9s| iter 200000] score = -18.34 ± 89.0303\n",
      "[elapsed time : 173.5s| iter 204000] score = -8.79 ± 15.6058\n",
      "[elapsed time : 176.8s| iter 208000] score = -31.37 ± 39.6381\n",
      "[elapsed time : 180.1s| iter 212000] score = -57.32 ± 42.5609\n",
      "[elapsed time : 184.8s| iter 216000] score = 10.30 ± 51.0577\n",
      "[elapsed time : 189.3s| iter 220000] score = -27.75 ± 15.9681\n",
      "[elapsed time : 194.1s| iter 224000] score = -7.43 ± 22.4910\n",
      "[elapsed time : 197.6s| iter 228000] score = -21.54 ± 25.3596\n",
      "[elapsed time : 202.3s| iter 232000] score = -28.77 ± 30.2865\n",
      "[elapsed time : 208.4s| iter 236000] score = -31.59 ± 37.4963\n",
      "[elapsed time : 215.1s| iter 240000] score = -21.01 ± 30.8059\n",
      "[elapsed time : 221.9s| iter 244000] score = -29.57 ± 51.4472\n",
      "[elapsed time : 225.9s| iter 248000] score = 1.62 ± 21.7056\n",
      "[elapsed time : 230.6s| iter 252000] score = -13.67 ± 58.9755\n",
      "[elapsed time : 234.7s| iter 256000] score = 4.36 ± 23.9943\n",
      "[elapsed time : 241.5s| iter 260000] score = 3.63 ± 43.7414\n",
      "[elapsed time : 252.3s| iter 264000] score = 31.41 ± 60.2280\n",
      "[elapsed time : 260.5s| iter 268000] score = -96.76 ± 89.8835\n",
      "[elapsed time : 269.2s| iter 272000] score = 1.34 ± 23.3492\n",
      "[elapsed time : 280.1s| iter 276000] score = -11.66 ± 62.5784\n",
      "[elapsed time : 285.9s| iter 280000] score = -0.82 ± 23.8048\n",
      "[elapsed time : 295.6s| iter 284000] score = -0.99 ± 28.4650\n",
      "[elapsed time : 300.6s| iter 288000] score = -25.76 ± 51.2094\n",
      "[elapsed time : 309.1s| iter 292000] score = 35.44 ± 53.4591\n",
      "[elapsed time : 316.7s| iter 296000] score = 41.06 ± 55.4349\n",
      "[elapsed time : 327.2s| iter 300000] score = -34.66 ± 84.5915\n",
      "[elapsed time : 333.8s| iter 304000] score = -88.61 ± 101.9642\n",
      "[elapsed time : 343.6s| iter 308000] score = 1.42 ± 71.6562\n",
      "[elapsed time : 351.7s| iter 312000] score = 0.64 ± 42.7029\n",
      "[elapsed time : 358.7s| iter 316000] score = 7.12 ± 52.0907\n",
      "[elapsed time : 367.7s| iter 320000] score = -2.82 ± 75.1407\n",
      "[elapsed time : 376.4s| iter 324000] score = 0.62 ± 111.2563\n",
      "[elapsed time : 383.2s| iter 328000] score = -47.14 ± 36.2367\n",
      "[elapsed time : 391.8s| iter 332000] score = -9.91 ± 86.0680\n",
      "[elapsed time : 402.4s| iter 336000] score = -18.15 ± 64.5127\n",
      "[elapsed time : 412.2s| iter 340000] score = 55.52 ± 68.8550\n",
      "[elapsed time : 419.4s| iter 344000] score = 8.39 ± 64.1806\n",
      "[elapsed time : 431.4s| iter 348000] score = 53.50 ± 87.6575\n",
      "[elapsed time : 441.9s| iter 352000] score = 11.29 ± 108.4577\n",
      "[elapsed time : 455.5s| iter 356000] score = 50.86 ± 80.3021\n",
      "[elapsed time : 469.3s| iter 360000] score = 84.34 ± 96.8391\n",
      "[elapsed time : 482.8s| iter 364000] score = 21.91 ± 189.9647\n",
      "[elapsed time : 498.7s| iter 368000] score = 78.35 ± 58.6361\n",
      "[elapsed time : 512.8s| iter 372000] score = 87.14 ± 67.4799\n",
      "[elapsed time : 527.0s| iter 376000] score = 121.51 ± 59.5803\n",
      "[elapsed time : 542.5s| iter 380000] score = 50.42 ± 67.8360\n",
      "[elapsed time : 556.4s| iter 384000] score = 81.86 ± 59.3219\n",
      "[elapsed time : 571.1s| iter 388000] score = 26.33 ± 54.3979\n",
      "[elapsed time : 585.2s| iter 392000] score = 69.90 ± 88.5096\n",
      "[elapsed time : 601.1s| iter 396000] score = 101.04 ± 30.2653\n",
      "[elapsed time : 616.8s| iter 400000] score = 76.88 ± 55.1459\n",
      "[elapsed time : 634.8s| iter 404000] score = 75.20 ± 46.5129\n",
      "[elapsed time : 650.7s| iter 408000] score = 73.55 ± 45.8182\n",
      "[elapsed time : 668.4s| iter 412000] score = 92.89 ± 36.8345\n",
      "[elapsed time : 684.4s| iter 416000] score = 59.38 ± 57.5504\n",
      "[elapsed time : 702.7s| iter 420000] score = 89.02 ± 44.6716\n",
      "[elapsed time : 722.1s| iter 424000] score = 88.79 ± 33.2806\n",
      "[elapsed time : 741.6s| iter 428000] score = 115.25 ± 31.3498\n",
      "[elapsed time : 761.1s| iter 432000] score = 58.81 ± 40.7791\n",
      "[elapsed time : 780.2s| iter 436000] score = 93.96 ± 24.3750\n",
      "[elapsed time : 799.5s| iter 440000] score = 40.35 ± 74.2000\n",
      "[elapsed time : 816.0s| iter 444000] score = 57.78 ± 79.3099\n",
      "[elapsed time : 834.2s| iter 448000] score = 77.62 ± 55.8665\n",
      "[elapsed time : 855.6s| iter 452000] score = 64.93 ± 62.5057\n",
      "[elapsed time : 872.9s| iter 456000] score = 54.86 ± 29.4658\n",
      "[elapsed time : 892.7s| iter 460000] score = 59.29 ± 12.4008\n",
      "[elapsed time : 909.1s| iter 464000] score = 80.52 ± 37.4651\n",
      "[elapsed time : 927.4s| iter 468000] score = 51.99 ± 11.1025\n",
      "[elapsed time : 944.3s| iter 472000] score = 75.21 ± 35.1194\n",
      "[elapsed time : 962.6s| iter 476000] score = 69.33 ± 11.7773\n",
      "[elapsed time : 984.6s| iter 480000] score = 68.58 ± 21.0473\n",
      "[elapsed time : 1005.6s| iter 484000] score = 26.07 ± 62.8466\n",
      "[elapsed time : 1025.7s| iter 488000] score = 48.24 ± 54.7189\n",
      "[elapsed time : 1048.4s| iter 492000] score = 20.77 ± 37.2507\n",
      "[elapsed time : 1068.1s| iter 496000] score = 62.86 ± 31.0910\n",
      "[elapsed time : 1087.0s| iter 500000] score = 17.87 ± 46.2357\n",
      "[elapsed time : 1106.9s| iter 504000] score = 19.05 ± 87.3438\n",
      "[elapsed time : 1125.7s| iter 508000] score = 41.23 ± 59.3988\n",
      "[elapsed time : 1145.6s| iter 512000] score = 48.70 ± 35.0270\n",
      "[elapsed time : 1166.0s| iter 516000] score = 32.19 ± 12.8709\n",
      "[elapsed time : 1185.1s| iter 520000] score = 58.50 ± 11.0925\n",
      "[elapsed time : 1204.8s| iter 524000] score = 28.38 ± 25.9195\n",
      "[elapsed time : 1227.8s| iter 528000] score = 46.31 ± 24.5258\n",
      "[elapsed time : 1249.5s| iter 532000] score = 13.95 ± 39.8699\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[elapsed time : 1272.7s| iter 536000] score = -1.06 ± 44.4518\n",
      "[elapsed time : 1292.9s| iter 540000] score = 52.14 ± 27.4368\n",
      "[elapsed time : 1311.6s| iter 544000] score = 51.36 ± 52.5124\n",
      "[elapsed time : 1327.1s| iter 548000] score = 6.20 ± 75.6901\n",
      "[elapsed time : 1345.0s| iter 552000] score = 19.65 ± 60.1252\n",
      "[elapsed time : 1363.1s| iter 556000] score = 28.47 ± 64.6490\n",
      "[elapsed time : 1384.6s| iter 560000] score = 3.58 ± 43.3114\n",
      "[elapsed time : 1406.7s| iter 564000] score = -3.18 ± 16.9393\n",
      "[elapsed time : 1427.5s| iter 568000] score = -2.01 ± 26.7813\n",
      "[elapsed time : 1448.2s| iter 572000] score = 0.52 ± 60.1744\n",
      "[elapsed time : 1467.7s| iter 576000] score = -17.85 ± 35.0557\n",
      "[elapsed time : 1487.6s| iter 580000] score = 7.40 ± 35.1258\n",
      "[elapsed time : 1509.6s| iter 584000] score = 2.05 ± 41.8522\n",
      "[elapsed time : 1528.8s| iter 588000] score = -42.14 ± 55.0282\n",
      "[elapsed time : 1549.0s| iter 592000] score = -33.80 ± 30.4635\n",
      "[elapsed time : 1567.5s| iter 596000] score = -20.94 ± 43.5944\n"
     ]
    }
   ],
   "source": [
    "train(env, agent, max_iter=600000, gamma=0.99, lr=3e-4, lam=0.95, steps_per_epoch=4000, eval_interval=4000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Watch how your agent solve the task!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score :  -22.608029337759753\n"
     ]
    }
   ],
   "source": [
    "# env = wrap_env(env)\n",
    "obs = env.reset()\n",
    "done = False\n",
    "score = 0.\n",
    "while not done:\n",
    "    # env.render()\n",
    "    obs, rew, done, _ = env.step(agent.act(obs)[0])\n",
    "    score += rew\n",
    "env.close()\n",
    "print('score : ', score)\n",
    "# show_video()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
