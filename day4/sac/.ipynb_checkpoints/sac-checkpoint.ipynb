{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SAC Practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import csv\n",
    "import gym\n",
    "import copy\n",
    "import os\n",
    "import numpy as np\n",
    "from collections import namedtuple\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from torch.distributions import Independent\n",
    "from torch.distributions.normal import Normal\n",
    "\n",
    "from utils import *\n",
    "from buffer import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current device = cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/core-dev/anaconda3/envs/cloudai/lib/python3.8/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: Unexpected error from cudaGetDeviceCount(). Did you run some cuda functions before calling NumCudaDevices() that might have already set an error? Error 803: system has unsupported display driver / cuda driver combination (Triggered internally at  /opt/conda/conda-bld/pytorch_1607370172916/work/c10/cuda/CUDAFunctions.cpp:100.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('current device =', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Define Q-network & policy-network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################\n",
    "##  Policy network with multi-layer perceptron  ##\n",
    "##################################################\n",
    "\n",
    "# Input - |S|\n",
    "# Output - normal distribution of size |A|\n",
    "\n",
    "class SACActor(nn.Module):\n",
    "    def __init__(self, dimS, dimA, hidden1, hidden2, ctrl_range):\n",
    "        super(SACActor, self).__init__()\n",
    "        self.fc1 = nn.Linear(dimS, hidden1)\n",
    "        self.fc2 = nn.Linear(hidden1, hidden2)\n",
    "        self.fc3 = nn.Linear(hidden2, dimA)\n",
    "        self.fc4 = nn.Linear(hidden2, dimA)\n",
    "\n",
    "        self.ctrl_range = ctrl_range\n",
    "\n",
    "    def forward(self, state, eval=False, with_log_prob=False):\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = F.relu(self.fc2(x))\n",
    "    \n",
    "        # Build normal distribution with parameters from layer\n",
    "        mu = self.fc3(x)\n",
    "        log_sigma = self.fc4(x)\n",
    "        \n",
    "        # clip value of log_sigma, as was done in Haarnoja's implementation of SAC:\n",
    "        # https://github.com/haarnoja/sac.git\n",
    "        log_sigma = torch.clamp(log_sigma, -20.0, 2.0)\n",
    "        \n",
    "        # Build normal distribution with parameters from layer\n",
    "        sigma = torch.exp(log_sigma)\n",
    "        distribution = Independent(Normal(mu, sigma), 1)\n",
    "\n",
    "        if not eval:\n",
    "            # use rsample() instead of sample(), as sample() does not allow back-propagation through params\n",
    "            u = distribution.rsample()\n",
    "            if with_log_prob:\n",
    "                log_prob = distribution.log_prob(u)\n",
    "                # Support?\n",
    "                log_prob -= 2.0 * torch.sum((np.log(2.0) + 0.5 * np.log(self.ctrl_range) - u - F.softplus(-2.0 * u)), dim=1)\n",
    "            else:\n",
    "                log_prob = None\n",
    "        # Give deterministic policy (centered at mu) when evaluation\n",
    "        else:\n",
    "            u = mu\n",
    "            log_prob = None\n",
    "            \n",
    "        # apply tanh so that the resulting action lies in (-1, 1)^D\n",
    "        # Squashed gaussian\n",
    "        a = self.ctrl_range * torch.tanh(u)\n",
    "\n",
    "        return a, log_prob\n",
    "    \n",
    "\n",
    "##################################################\n",
    "##  Critic network with multi-layer perceptron  ##\n",
    "##################################################\n",
    "\n",
    "# Input - |S|+|A|\n",
    "# Output - single value\n",
    "\n",
    "class DoubleCritic(nn.Module):\n",
    "    # Retain double network - Idea from TD3\n",
    "    def __init__(self, dimS, dimA, hidden1, hidden2):\n",
    "        super(DoubleCritic, self).__init__()    \n",
    "        # Q1\n",
    "        self.fc1 = nn.Linear(dimS + dimA, hidden1)\n",
    "        self.fc2 = nn.Linear(hidden1, hidden2)\n",
    "        self.fc3 = nn.Linear(hidden2, 1)\n",
    "        \n",
    "        # Q2\n",
    "        self.fc4 = nn.Linear(dimS + dimA, hidden1)\n",
    "        self.fc5 = nn.Linear(hidden1, hidden2)\n",
    "        self.fc6 = nn.Linear(hidden2, 1)\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        x = torch.cat([state, action], dim=1)\n",
    "        \n",
    "        # Q1\n",
    "        x1 = F.relu(self.fc1(x))\n",
    "        x1 = F.relu(self.fc2(x1))\n",
    "        x1 = self.fc3(x1)\n",
    "        \n",
    "        # Q2\n",
    "        x2 = F.relu(self.fc4(x))\n",
    "        x2 = F.relu(self.fc5(x2))\n",
    "        x2 = self.fc6(x2)\n",
    "\n",
    "        return x1, x2\n",
    "\n",
    "    def Q1(self, state, action):\n",
    "        x = torch.cat([state, action], dim=1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Define SAC agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SACAgent:\n",
    "    def __init__(self,\n",
    "                 dimS,\n",
    "                 dimA,\n",
    "                 ctrl_range,\n",
    "                 gamma=0.99,\n",
    "                 pi_lr=1e-4,\n",
    "                 q_lr=1e-3,\n",
    "                 polyak=1e-3,\n",
    "                 alpha=0.2,\n",
    "                 hidden1=400,\n",
    "                 hidden2=300,\n",
    "                 buffer_size=1000000,\n",
    "                 batch_size=128,\n",
    "                 device='cpu',\n",
    "                 render=False):\n",
    "\n",
    "        self.dimS = dimS\n",
    "        self.dimA = dimA\n",
    "        self.ctrl_range = ctrl_range\n",
    "\n",
    "        self.gamma = gamma\n",
    "        self.pi_lr = pi_lr\n",
    "        self.q_lr = q_lr\n",
    "        self.polyak = polyak\n",
    "        self.alpha = alpha\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        # networks definition\n",
    "        # pi : actor network, Q : 2 critic network\n",
    "        self.pi = SACActor(dimS, dimA, hidden1, hidden2, ctrl_range).to(device)\n",
    "        self.Q = DoubleCritic(dimS, dimA, hidden1, hidden2).to(device)\n",
    "\n",
    "        # target networks\n",
    "        self.target_Q = copy.deepcopy(self.Q).to(device)\n",
    "        freeze(self.target_Q)\n",
    "\n",
    "        self.buffer = ReplayBuffer(dimS, dimA, limit=buffer_size)\n",
    "\n",
    "        self.Q_optimizer = Adam(self.Q.parameters(), lr=self.q_lr)\n",
    "        self.pi_optimizer = Adam(self.pi.parameters(), lr=self.pi_lr)\n",
    "\n",
    "        self.device = device\n",
    "        self.render = render\n",
    "\n",
    "        return\n",
    "\n",
    "    \n",
    "    def act(self, state, eval=False):\n",
    "\n",
    "        state = torch.tensor(state, dtype=torch.float).to(self.device)\n",
    "        with torch.no_grad():\n",
    "            action, _ = self.pi(state, eval=eval, with_log_prob=False)\n",
    "        action = action.cpu().detach().numpy()\n",
    "\n",
    "        return action\n",
    "    \n",
    "    \n",
    "    def target_update(self):\n",
    "\n",
    "        for params, target_params in zip(self.Q.parameters(), self.target_Q.parameters()):\n",
    "            target_params.data.copy_(self.polyak * params.data + (1.0 - self.polyak) * target_params.data)\n",
    "\n",
    "        return\n",
    "    \n",
    "    \n",
    "    def save_model(self, path):\n",
    "        print('adding checkpoints...')\n",
    "        checkpoint_path = path + 'model.pth.tar'\n",
    "        torch.save(\n",
    "                    {'actor': self.pi.state_dict(),\n",
    "                     'critic': self.Q.state_dict(),\n",
    "                     'target_critic': self.target_Q.state_dict(),\n",
    "                     'actor_optimizer': self.pi_optimizer.state_dict(),\n",
    "                     'critic_optimizer': self.Q_optimizer.state_dict()\n",
    "                    },\n",
    "                    checkpoint_path)\n",
    "\n",
    "        return\n",
    "\n",
    "    \n",
    "    def load_model(self, path):\n",
    "        print('networks loading...')\n",
    "        checkpoint = torch.load(path)\n",
    "\n",
    "        self.pi.load_state_dict(checkpoint['actor'])\n",
    "        self.Q.load_state_dict(checkpoint['critic'])\n",
    "        self.target_Q.load_state_dict(checkpoint['target_critic'])\n",
    "        self.pi_optimizer.load_state_dict(checkpoint['actor_optimizer'])\n",
    "        self.Q_optimizer.load_state_dict(checkpoint['critic_optimizer'])\n",
    "\n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Implement one-step param update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update(agent, batch):\n",
    "    # Upload batch to GPU\n",
    "    obs_batch = torch.tensor(batch.obs, dtype=torch.float).to(device)\n",
    "    act_batch = torch.tensor(batch.act, dtype=torch.float).to(device)\n",
    "    next_obs_batch = torch.tensor(batch.next_obs, dtype=torch.float).to(device)\n",
    "    rew_batch = torch.tensor(batch.rew, dtype=torch.float).to(device)\n",
    "    done_batch = torch.tensor(batch.done, dtype=torch.float).to(device)\n",
    "    masks = torch.tensor([1.]) - done_batch\n",
    "    \n",
    "    #########################\n",
    "    ##    Critic Update    ##\n",
    "    #########################\n",
    "    # Build Bellman target\n",
    "    with torch.no_grad():\n",
    "        # Get action with log(pi(a|s)) (also gradient)\n",
    "        next_actions, log_probs = agent.pi(next_obs_batch, with_log_prob=True)\n",
    "        \n",
    "        # To calculate TQ, we need Q(s',pi(s'))\n",
    "        target_q1, target_q2 = agent.target_Q(next_obs_batch, next_actions)\n",
    "        \n",
    "        # To mitigate overestimation! - Idea from TD3\n",
    "        target_q = torch.min(target_q1, target_q2)\n",
    "        \n",
    "        # TQ^pi = r + gamma [ Q(s',pi(s')) - alpha H(pi(s')) ]\n",
    "        # Recall : H = sum[ -P(X) * log(P(x)) ] = E [ -log(P(x)) ]\n",
    "        # Recall : H \\approx -log(P(x))\n",
    "        TQ = rew_batch + agent.gamma * masks * (target_q - agent.alpha * log_probs)\n",
    "\n",
    "    # Calculate MSELoss\n",
    "    Q1, Q2 = self.Q(obs_batch, act_batch)\n",
    "    Q_loss1 = torch.mean((Q1 - TQ)**2)\n",
    "    Q_loss2 = torch.mean((Q2 - TQ)**2)\n",
    "    Q_loss = Q_loss1 + Q_loss2\n",
    "\n",
    "    # Gradient descent\n",
    "    self.Q_optimizer.zero_grad()\n",
    "    Q_loss.backward()\n",
    "    self.Q_optimizer.step()\n",
    "    \n",
    "    ########################\n",
    "    ##    Actor Update    ##\n",
    "    ########################\n",
    "    actions, log_probs = self.pi(obs_batch, with_log_prob=True)\n",
    "    \n",
    "    freeze(self.Q)\n",
    "    q1, q2 = self.Q(obs_batch, actions)\n",
    "    q = torch.min(q1, q2)\n",
    "\n",
    "    # Need to perform gradient ascent, so (-) is required\n",
    "    pi_loss = torch.mean(q - self.alpha * log_probs)\n",
    "    pi_loss = - pi_loss\n",
    "    \n",
    "    # Gradient ascent\n",
    "    self.pi_optimizer.zero_grad()\n",
    "    pi_loss.backward()\n",
    "    self.pi_optimizer.step()\n",
    "    \n",
    "    ####################################\n",
    "    ##    Soft Target Critic Update    #\n",
    "    ####################################\n",
    "    unfreeze(self.Q)\n",
    "    self.target_update()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Putting these together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_agent(agent, env_id):\n",
    "    eval_agent(agent, env_id, eval_num=1, render=True)\n",
    "\n",
    "\n",
    "def eval_agent(agent, env_id, eval_num=5, render=False):\n",
    "    log = []\n",
    "    for ep in range(eval_num):\n",
    "        env = gym.make(env_id)\n",
    "\n",
    "        state = env.reset()\n",
    "        step_count = 0\n",
    "        ep_reward = 0\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            if render and ep == 0:\n",
    "                env.render()\n",
    "\n",
    "            action = agent.act(state, eval=True)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            step_count += 1\n",
    "            state = next_state\n",
    "            ep_reward += reward\n",
    "\n",
    "        if render and ep == 0:\n",
    "            env.close()\n",
    "        log.append(ep_reward)\n",
    "\n",
    "    avg = sum(log) / eval_num\n",
    "\n",
    "    return avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_sac(\n",
    "            env_id,\n",
    "            max_iter=1e6,\n",
    "            eval_interval=2000,\n",
    "            start_train=10000,\n",
    "            train_interval=50,\n",
    "            buffer_size=1e6,\n",
    "            fill_buffer=20000,\n",
    "            truncate=1000,\n",
    "            gamma=0.99,\n",
    "            pi_lr=3e-4,\n",
    "            q_lr=3e-4,\n",
    "            polyak=5e-3,\n",
    "            alpha=0.2,\n",
    "            hidden1=256,\n",
    "            hidden2=256,\n",
    "            batch_size=128,\n",
    "            device='cpu',\n",
    "            render='False'\n",
    "            ):\n",
    "\n",
    "    params = locals()\n",
    "\n",
    "    max_iter = int(max_iter)\n",
    "    buffer_size = int(buffer_size)\n",
    "    env = gym.make(env_id)\n",
    "\n",
    "    dimS, dimA, ctrl_range, max_ep_len = get_env_spec(env)\n",
    "\n",
    "    if truncate is not None:\n",
    "        max_ep_len = truncate\n",
    "\n",
    "    # Instantize agent\n",
    "    agent = SACAgent(\n",
    "                     dimS,\n",
    "                     dimA,\n",
    "                     ctrl_range,\n",
    "                     gamma=gamma,\n",
    "                     pi_lr=pi_lr,\n",
    "                     q_lr=q_lr,\n",
    "                     polyak=polyak,\n",
    "                     alpha=alpha,\n",
    "                     hidden1=hidden1,\n",
    "                     hidden2=hidden2,\n",
    "                     buffer_size=buffer_size,\n",
    "                     batch_size=batch_size,\n",
    "                     device=device,\n",
    "                     render=render\n",
    "                     )\n",
    "\n",
    "    set_log_dir(env_id)\n",
    "    \n",
    "    # Logging & Saving Weights\n",
    "    num_checkpoints = 5\n",
    "    checkpoint_interval = max_iter // (num_checkpoints - 1)\n",
    "    current_time = time.strftime(\"%m%d-%H%M%S\")\n",
    "    train_log = open('./train_log/' + env_id + '/SAC_' + current_time + '.csv',\n",
    "                     'w', encoding='utf-8', newline='')\n",
    "\n",
    "    path = './eval_log/' + env_id + '/SAC_' + current_time\n",
    "    eval_log = open(path + '.csv', 'w', encoding='utf-8', newline='')\n",
    "\n",
    "    train_logger = csv.writer(train_log)\n",
    "    eval_logger = csv.writer(eval_log)\n",
    "\n",
    "    with open(path + '.txt', 'w') as f:\n",
    "        for key, val in params.items():\n",
    "            print(key, '=', val, file=f)\n",
    "\n",
    "    ##############################\n",
    "    ##    Main training loop    ##\n",
    "    ##############################\n",
    "    obs = env.reset()\n",
    "    step_count, ep_reward = 0, 0\n",
    "    start = time.time()\n",
    "    \n",
    "    for t in range(max_iter + 1):\n",
    "        # Rollout agent to fill in replay buffer\n",
    "        if t < fill_buffer:\n",
    "            # For early stage of training, use random agent to promote exploration\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            action = agent.act(obs)\n",
    "\n",
    "        next_obs, reward, done, _ = env.step(action)\n",
    "        step_count += 1\n",
    "\n",
    "        if step_count == max_ep_len:\n",
    "            done = False\n",
    "\n",
    "        agent.buffer.append(obs, action, next_obs, reward, done)\n",
    "\n",
    "        obs = next_obs\n",
    "        ep_reward += reward\n",
    "        \n",
    "        # Reset environment if trajectory ends\n",
    "        if done or (step_count == max_ep_len):\n",
    "            train_logger.writerow([t, ep_reward])\n",
    "            obs = env.reset()\n",
    "            step_count, ep_reward = 0\n",
    "        \n",
    "        # Actor-Critic\n",
    "        if (t >= start_train) and (t % train_interval == 0):\n",
    "            # Iterate sampling batch and updating actor-critic\n",
    "            for _ in range(train_interval):\n",
    "                batch = agent.buffer.sample_batch(batch_size=batch_size)\n",
    "                update(agent, batch)\n",
    "        \n",
    "        # Evaluate agent\n",
    "        if t % eval_interval == 0:\n",
    "            eval_score = eval_agent(agent, env_id, render=False)\n",
    "            log = [t, eval_score]\n",
    "            print('step {} : {:.4f}'.format(t, eval_score))\n",
    "            eval_logger.writerow(log)\n",
    "        \n",
    "        \"\"\"\n",
    "        # Render agent peridoically\n",
    "        if t % (10 * eval_interval) == 0:\n",
    "            if render:\n",
    "                render_agent(agent, env_id)\n",
    "        \"\"\"\n",
    "        \n",
    "        # Save agent weight while training\n",
    "        if t % checkpoint_interval == 0:\n",
    "            agent.save_model('./checkpoints/' + env_id + '/sac_{}th_iter_'.format(t))\n",
    "\n",
    "    train_log.close()\n",
    "    eval_log.close()\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Let's train our agent!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use continuous control!\n",
    "env_id = 'LunarLanderContinuous-v2'\n",
    "truncate = 1000\n",
    "max_iter = 5e5\n",
    "eval_interval = 2000\n",
    "render = False\n",
    "tau = 5e-3\n",
    "lr = 3e-4\n",
    "hidden1 = 256\n",
    "hidden2 = 256\n",
    "train_interval = 50\n",
    "start_train = 1e4\n",
    "fill_buffer = 2e4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "observation space dim. : 8 / # actions : 2\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(env_id)\n",
    "obs_dim = env.observation_space.shape[0]\n",
    "num_act = env.action_space.shape[0]\n",
    "\n",
    "print('observation space dim. : {} / # actions : {}'.format(obs_dim, num_act))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run experiment!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "environment : LunarLanderContinuous-v2\n",
      "obs dim :  (8,) / ctrl dim :  (2,)\n",
      "--------------------------------------------------------------------------------\n",
      "ctrl range : (-1.00, 1.00)\n",
      "max_ep_len :  1000\n",
      "--------------------------------------------------------------------------------\n",
      "step 0 : -287.4919\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'SACAgent' object has no attribute 'save_model'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-b0cfaa848a13>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m run_sac(\n\u001b[0m\u001b[1;32m      2\u001b[0m         \u001b[0menv_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0mmax_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0meval_interval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meval_interval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mstart_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstart_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-478404a3f0e3>\u001b[0m in \u001b[0;36mrun_sac\u001b[0;34m(env_id, max_iter, eval_interval, start_train, train_interval, buffer_size, fill_buffer, truncate, gamma, pi_lr, q_lr, polyak, alpha, hidden1, hidden2, batch_size, device, render)\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0;31m# Save agent weight while training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mcheckpoint_interval\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m             \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./checkpoints/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0menv_id\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/sac_{}th_iter_'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0mtrain_log\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'SACAgent' object has no attribute 'save_model'"
     ]
    }
   ],
   "source": [
    "run_sac(\n",
    "        env_id,\n",
    "        max_iter=max_iter,\n",
    "        eval_interval=eval_interval,\n",
    "        start_train=start_train,\n",
    "        train_interval=train_interval,\n",
    "        fill_buffer=fill_buffer,\n",
    "        truncate=truncate,\n",
    "        gamma=0.99,\n",
    "        pi_lr=lr,\n",
    "        q_lr=lr,\n",
    "        polyak=tau,\n",
    "        alpha=0.2,\n",
    "        hidden1=hidden1,\n",
    "        hidden2=hidden2,\n",
    "        batch_size=128,\n",
    "        buffer_size=1e6,\n",
    "        device=device,\n",
    "        render=render\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Watch the trained agent!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = env.reset()\n",
    "done = False\n",
    "score = 0.\n",
    "load_model(agent, path='./snapshots/trained.pth.tar', device=device)\n",
    "while not done:\n",
    "    env.render()\n",
    "    obs, rew, done, _ = env.step(agent.act(obs))\n",
    "    score += rew\n",
    "    \n",
    "env.close()\n",
    "print('score : ', score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
