{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import MSELoss\n",
    "import torch.nn.functional as F\n",
    "import copy\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch.optim import Adam\n",
    "from buffer import ReplayBuffer\n",
    "from utils import save_snapshot, recover_snapshot, load_model\n",
    "import gym\n",
    "import pybulletgym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current device = cuda\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('current device =', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Define Q-network & policy-network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# critic network definition\n",
    "# multi-layer perceptron (with 2 hidden layers)\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, obs_dim, act_dim, hidden1, hidden2):\n",
    "        super(Critic, self).__init__()\n",
    "        self.fc1 = nn.Linear(obs_dim + act_dim, hidden1)\n",
    "        self.fc2 = nn.Linear(hidden1, hidden2)\n",
    "        self.fc3 = nn.Linear(hidden2, 1)\n",
    "        \n",
    "    \n",
    "    def forward(self, obs, act):\n",
    "        x = torch.cat([obs, act], dim=1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        \n",
    "        return self.fc3(x)\n",
    "    \n",
    "    \n",
    "# actor network definition\n",
    "# multi-layer perceptron (with 2 hidden layers)\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, obs_dim, act_dim, ctrl_range, hidden1, hidden2):\n",
    "        super(Actor, self).__init__()\n",
    "        self.fc1 = nn.Linear(obs_dim, hidden1)\n",
    "        self.fc2 = nn.Linear(hidden1, hidden2)\n",
    "        self.fc3 = nn.Linear(hidden2, act_dim)\n",
    "        self.ctrl_range = ctrl_range\n",
    "        \n",
    "    def forward(self, obs):\n",
    "        x = F.relu(self.fc1(obs))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        \n",
    "        return self.ctrl_range * torch.tanh(self.fc3(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Action Constraints?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('Pendulum-v0')\n",
    "lb = env.action_space.high\n",
    "ub = env.action_space.low\n",
    "print('minimum torque available = ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Define DDPG agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDPGAgent:\n",
    "    def __init__(self, obs_dim, act_dim, ctrl_range, hidden1, hidden2):\n",
    "        \n",
    "        # networks\n",
    "        self.actor = Actor(obs_dim, act_dim, ctrl_range, hidden1, hidden2).to(device)\n",
    "        self.critic = Critic(obs_dim, act_dim, hidden1, hidden2).to(device)\n",
    "                \n",
    "    def act(self, obs):\n",
    "        # numpy ndarray to torch tensor\n",
    "        # we first add an extra dimension\n",
    "        obs = obs[np.newaxis, ...]\n",
    "        with torch.no_grad():\n",
    "            obs_tensor = torch.Tensor(obs).to(device)\n",
    "            act_tensor = self.actor(obs_tensor)\n",
    "\n",
    "        # torch tensor to numpy ndarray\n",
    "        # remove extra dimension\n",
    "        action = act_tensor.cpu().detach().numpy()\n",
    "        action = np.squeeze(action, axis=0)\n",
    "        \n",
    "        return action\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1.Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-2.0849288  0.6340436]\n"
     ]
    }
   ],
   "source": [
    "agent = DDPGAgent(4, 2, 3, 32, 32)\n",
    "action = agent.act(np.array([3., -1., 2., -5.]))\n",
    "print(action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Implement one-step param update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update(agent, replay_buf, gamma, actor_optim, critic_optim, target_actor, target_critic, tau, batch_size):\n",
    "    # agent : agent with networks to be trained\n",
    "    # replay_buf : replay buf from which we sample a batch\n",
    "    # actor_optim / critic_optim : torch optimizers\n",
    "    # tau : parameter for soft target update\n",
    "    \n",
    "    batch = replay_buf.sample_batch(batch_size=batch_size)\n",
    "\n",
    "    # target construction does not need backward ftns\n",
    "    with torch.no_grad():\n",
    "        # unroll batch\n",
    "        obs = torch.Tensor(batch.obs).to(device)\n",
    "        act = torch.Tensor(batch.act).to(device)\n",
    "        next_obs = torch.Tensor(batch.next_obs).to(device)\n",
    "        rew = torch.Tensor(batch.rew).to(device)\n",
    "        done = torch.Tensor(batch.done).to(device)\n",
    "        \n",
    "        ################\n",
    "        # train critic #\n",
    "        ################\n",
    "        mask = 1. - done\n",
    "        target = rew + gamma * mask * target_critic(next_obs, target_actor(next_obs))\n",
    "    \n",
    "    out = agent.critic(obs, act)\n",
    "    \n",
    "    loss_ftn = MSELoss()\n",
    "    critic_loss = loss_ftn(out, target)\n",
    "    # alternative : loss = torch.mean((target - out)**2)\n",
    "    \n",
    "    critic_optim.zero_grad()\n",
    "    critic_loss.backward()\n",
    "    critic_optim.step()\n",
    "    \n",
    "    ###############\n",
    "    # train actor #\n",
    "    ###############\n",
    "    \n",
    "    # freeze critic during actor training (why?)\n",
    "    for p in agent.critic.parameters():\n",
    "        p.requires_grad_(False)\n",
    "    \n",
    "    actor_loss = -torch.mean(agent.critic(obs, agent.actor(obs)))\n",
    "    \n",
    "    actor_optim.zero_grad()\n",
    "    actor_loss.backward()\n",
    "    actor_optim.step()\n",
    "    \n",
    "    \n",
    "    # unfreeze critic after actor training\n",
    "    for p in agent.critic.parameters():\n",
    "        p.requires_grad_(True)\n",
    "        \n",
    "    # soft target update (both actor & critic network)\n",
    "    for p, targ_p in zip(agent.actor.parameters(), target_actor.parameters()):\n",
    "        targ_p.data.copy_((1. - tau) * targ_p + tau * p)\n",
    "    for p, targ_p in zip(agent.critic.parameters(), target_critic.parameters()):\n",
    "        targ_p.data.copy_((1. - tau) * targ_p + tau * p)\n",
    "        \n",
    "        \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(agent, env, num_episodes=5):\n",
    "\n",
    "    sum_scores = 0.\n",
    "    \n",
    "    for i in range(num_episodes):\n",
    "        obs = env.reset()\n",
    "        done = False\n",
    "        score = 0.\n",
    "        \n",
    "        while not done:\n",
    "            action = agent.act(obs)\n",
    "            obs, rew, done, _ = env.step(action)\n",
    "            score += rew\n",
    "        sum_scores += score\n",
    "    avg_score = sum_scores / num_episodes\n",
    "    \n",
    "    return avg_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Combining these, we finally have..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(agent, env, gamma, \n",
    "          actor_lr, critic_lr, tau, noise_std,\n",
    "          ep_len, num_updates, batch_size,\n",
    "          init_buffer=5000, buffer_size=100000,\n",
    "          start_train=2000, train_interval=50,\n",
    "          eval_interval=2000, snapshot_interval=10000, path=None):\n",
    "    \n",
    "    target_actor = copy.deepcopy(agent.actor)\n",
    "    target_critic = copy.deepcopy(agent.critic)\n",
    "    \n",
    "    # freeze target networks\n",
    "    for p in target_actor.parameters():\n",
    "        p.requires_grad_(False)\n",
    "    for p in target_critic.parameters():\n",
    "        p.requires_grad_(False)\n",
    "\n",
    "    actor_optim = Adam(agent.actor.parameters(), lr=actor_lr)\n",
    "    critic_optim = Adam(agent.critic.parameters(), lr=critic_lr)\n",
    "    \n",
    "    if path is not None:\n",
    "        recover_snapshot(path, agent.actor, agent.critic,\n",
    "                   target_actor, target_critic,\n",
    "                   actor_optim, critic_optim,\n",
    "                   device=device\n",
    "                  )\n",
    "        # load snapshot\n",
    "    \n",
    "    obs_dim = env.observation_space.shape[0]\n",
    "    act_dim = env.action_space.shape[0]\n",
    "    ctrl_range = env.action_space.high[0]\n",
    "    \n",
    "    replay_buf = ReplayBuffer(obs_dim, act_dim, buffer_size)\n",
    "    \n",
    "    save_path = './snapshots/'\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    \n",
    "    # main loop\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    step_count = 0\n",
    "    \n",
    "    for t in tqdm(range(num_updates + 1)):\n",
    "        if t < init_buffer:\n",
    "            # perform random action until we collect sufficiently many samples\n",
    "            # this is for exploration purpose\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            # executes noisy action\n",
    "            # a_t = \\pi(s_t) + N(0, \\sigma^2)\n",
    "            action = agent.act(obs) + noise_std * np.random.randn(act_dim)\n",
    "            action = np.clip(action, -ctrl_range, ctrl_range)\n",
    "            \n",
    "        next_obs, rew, done, _ = env.step(action)\n",
    "        step_count += 1\n",
    "        if step_count == ep_len:\n",
    "            # if the next_state is not terminal but done is set to True by gym env wrapper\n",
    "            done = False\n",
    "            \n",
    "        replay_buf.append(obs, action, next_obs, rew, done)\n",
    "        obs = next_obs\n",
    "        \n",
    "        if done == True or step_count == ep_len:\n",
    "            # reset environment if current environment reaches a terminal state \n",
    "            # or step count reaches predefined length\n",
    "            obs = env.reset()\n",
    "            done = False\n",
    "            step_count = 0\n",
    "            # score = evaluate(agent, env)\n",
    "            # print('[iteration {}] evaluation score : {}'.format(t, score))\n",
    "        \n",
    "        \n",
    "        if t > start_train and t % train_interval == 0:\n",
    "            # start training after fixed number of steps\n",
    "            # this may mitigate overfitting of networks to the \n",
    "            # small number of samples collected during the initial stage of training\n",
    "            for _ in range(train_interval):\n",
    "                update(agent,\n",
    "                       replay_buf,\n",
    "                       gamma,\n",
    "                       actor_optim,\n",
    "                       critic_optim,\n",
    "                       target_actor,\n",
    "                       target_critic,\n",
    "                       tau,\n",
    "                       batch_size\n",
    "                      )\n",
    "        if t % snapshot_interval == 0:\n",
    "            snapshot_path = save_path + 'iter{}_'.format(t)\n",
    "            # save weight & training progress\n",
    "            save_snapshot(snapshot_path, agent.actor, agent.critic,\n",
    "                          target_actor, target_critic,\n",
    "                          actor_optim, critic_optim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Let's test the code!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "observation space dim : 3 / action space dim : 1\n",
      "ctrl range :  2.0\n"
     ]
    }
   ],
   "source": [
    "# env = gym.make('HalfCheetahPyBulletEnv-v0')\n",
    "env = gym.make('Pendulum-v0')\n",
    "obs_dim = env.observation_space.shape[0]\n",
    "act_dim = env.action_space.shape[0]\n",
    "ctrl_range = env.action_space.high[0]\n",
    "\n",
    "print('observation space dim : {} / action space dim : {}'.format(obs_dim, act_dim))\n",
    "print('ctrl range : ', ctrl_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = DDPGAgent(obs_dim=obs_dim, act_dim=act_dim, ctrl_range=ctrl_range, hidden1=256, hidden2=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.99\n",
    "actor_lr = 1e-4\n",
    "critic_lr = 1e-3\n",
    "tau = 1e-3\n",
    "noise_std = 0.1\n",
    "ep_len = 1000\n",
    "num_updates = 10000\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000001/1000001 [1:43:43<00:00, 160.68it/s] \n"
     ]
    }
   ],
   "source": [
    "train(agent, env, gamma,\n",
    "      actor_lr, critic_lr, tau, noise_std,\n",
    "      ep_len, num_updates, batch_size,\n",
    "      init_buffer=5000, buffer_size=100000,\n",
    "      start_train=2000, train_interval=50,\n",
    "      eval_interval=5000\n",
    "     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Watch the trained agent!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('HalfCheetahPyBulletEnv-v0')\n",
    "env.render(mode='human')\n",
    "obs = env.reset()\n",
    "done = False\n",
    "score = 0.\n",
    "\n",
    "load_model(agent, path='./snapshots/iter370000_model.pth.tar', device=device)\n",
    "\n",
    "for _ in range(1000):\n",
    "    env.render(mode='human')\n",
    "    obs, rew, _, _ = env.step(agent.act(obs))\n",
    "    score += rew\n",
    "print('score : ', score)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
