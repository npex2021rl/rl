{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REINFORCE and Actor-Critic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Setups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import time\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "from sklearn.linear_model import LinearRegression, Lasso\n",
    "\n",
    "# For Colab users, turn this into true\n",
    "colab = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select hardware to use - GPU or CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/core-dev/anaconda3/envs/cloudai/lib/python3.8/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: Unexpected error from cudaGetDeviceCount(). Did you run some cuda functions before calling NumCudaDevices() that might have already set an error? Error 803: system has unsupported display driver / cuda driver combination (Triggered internally at  /opt/conda/conda-bld/pytorch_1607370172916/work/c10/cuda/CUDAFunctions.cpp:100.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device = 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For rendering **[COLAB USE ONLY!]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if colab:\n",
    "    !pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
    "    !apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1\n",
    "    !apt-get update > /dev/null 2>&1\n",
    "    !apt-get install cmake > /dev/null 2>&1\n",
    "    !pip install --upgrade setuptools 2>&1\n",
    "    !pip install ez_setup > /dev/null 2>&1\n",
    "    !pip3 install box2d-py\n",
    "    !pip3 install gym[Box_2D]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build Environment and check MDP size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimension of state space / number of actions : 4 / 2\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "env.seed(500)\n",
    "torch.manual_seed(500)\n",
    "\n",
    "# Configure MDP\n",
    "gamma = 0.99\n",
    "state_dim = env.observation_space.low.size\n",
    "num_action = env.action_space.n\n",
    "print('Dimension of state space / number of actions : %d / %d'%(state_dim, num_action))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Create an policy and value function instance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Define policy network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy(nn.Module):\n",
    "    def __init__(self, state_dim, num_action, hidden_size1, hidden_size2):\n",
    "        super(Policy, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim, hidden_size1)\n",
    "        self.fc2 = nn.Linear(hidden_size1, hidden_size2)\n",
    "        self.fc3 = nn.Linear(hidden_size2, num_action)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        action_score = self.fc3(x)\n",
    "        return F.softmax(action_score, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. REINFORCE loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python \n",
    "m = Categorial(probs)\n",
    "```\n",
    "makes neural network output computation graph (gradient) into discrete probability distribution, thus it is possible to calculate $\\nabla_\\theta\\log{\\pi_\\theta(a|s)}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(state):\n",
    "    state = torch.from_numpy(state).float().unsqueeze(0)\n",
    "    state = state.to(device)\n",
    "    probs = pi(state)\n",
    "    \n",
    "    m = Categorical(probs)\n",
    "    action = m.sample()\n",
    "    \n",
    "    return action.item(), m.log_prob(action)\n",
    "\n",
    "def sample_trajectory(data, T):\n",
    "    # Reset environment to get new trajectory\n",
    "    state = env.reset()\n",
    "    r_sum, r_sum_discount = 0, 0\n",
    "    \n",
    "    for t in range(T):\n",
    "        # Get action from current policy and rollout\n",
    "        action, log_prob = select_action(state)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        r_sum += reward\n",
    "        r_sum_discount += reward * (gamma ** t) \n",
    "\n",
    "        # Store data\n",
    "        data['log_pi'].append(-log_prob) # (-) sign for gradient ascent\n",
    "        data['state'].append(state)\n",
    "        data['next_state'].append(next_state)\n",
    "        data['reward'].append(reward)\n",
    "        \n",
    "        # Step\n",
    "        state = next_state\n",
    "        if done:\n",
    "            break\n",
    "    \n",
    "    return r_sum, r_sum_discount"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "REINFORCE algorithm approximate gradient for policy parameter $\\theta$ with sampled trajectory\n",
    "$$ \\nabla_\\theta J(\\theta) \\approx \\frac{1}{N} \\sum^N_{i=1} \\big( \\sum^T_{t=0}\\nabla_\\theta\\log{\\pi_\\theta}(a_t|s_t) \\big) \\big( \\sum^T_{t=0} \\gamma^t r(s_t,a_t) \\big)$$\n",
    "\n",
    "With further approximation and use of baseline,\n",
    "$$ \\nabla_\\theta J(\\theta) \\approx \\frac{1}{N} \\sum^N_{i=1} \\sum^T_{t=0} \\big( \\nabla_\\theta\\log{\\pi_\\theta(a_t|s_t)} \\big) \\big( Q(s_t,a_t) - v(s_t) \\big)$$\n",
    "\n",
    "For REINFORCE, we use $Q(s_t, a_t) \\approx \\sum^T_{t'=t} \\gamma^t r(s_{t'}, a_{t'})$, baseline $v^{\\pi_\\theta}(s_0) \\approx \\mathbb{E}_{s_0, \\pi_\\theta} \\big[ \\sum^T_{t=0} \\gamma^t r_t \\big]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_PG(pi_returns_discounted, dataset):\n",
    "    pi_loss = 0\n",
    "    for data in dataset:\n",
    "        advantage, DCR = [], 0\n",
    "        for r in reversed(data['reward']):\n",
    "            # TODO : Caculate discounted redataset.append(data)turn from t=i\n",
    "            # Hint : reversed() will give saved rewards in reversed order\n",
    "            #DCR =\n",
    "            \n",
    "            # Q(s,a) is replaced with discounted sum of rewards (DCR)\n",
    "            # v(s) is replaced with empirical v(s_0)\n",
    "            advantage.insert(0, DCR - np.mean(pi_returns_discounted))\n",
    "\n",
    "        # TODO : alternate between two losses to see difference!\n",
    "        pi_loss_vanilla = [log_pi * DCR for log_pi in data['log_pi']]\n",
    "        pi_loss_baseline = [log_pi * a for log_pi, a in zip(data['log_pi'], advantage)]\n",
    "        \n",
    "        # Take mean value\n",
    "        pi_loss += torch.cat(pi_loss_baseline).sum()\n",
    "        \n",
    "    return pi_loss / num_trajs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\tReturn_mean: 19.76\tReturn_std: 8.49\tTime(Elapsed/Remain): 0.05/5.17 (mins)\n",
      "Epoch 5\tReturn_mean: 29.37\tReturn_std: 18.32\tTime(Elapsed/Remain): 0.24/3.73 (mins)\n",
      "Epoch 10\tReturn_mean: 40.56\tReturn_std: 25.14\tTime(Elapsed/Remain): 0.46/3.71 (mins)\n",
      "Epoch 15\tReturn_mean: 53.07\tReturn_std: 26.29\tTime(Elapsed/Remain): 0.75/3.95 (mins)\n",
      "Epoch 20\tReturn_mean: 76.31\tReturn_std: 44.41\tTime(Elapsed/Remain): 1.15/4.32 (mins)\n",
      "Epoch 25\tReturn_mean: 131.40\tReturn_std: 71.74\tTime(Elapsed/Remain): 1.77/5.05 (mins)\n",
      "Epoch 30\tReturn_mean: 192.59\tReturn_std: 94.73\tTime(Elapsed/Remain): 2.79/6.21 (mins)\n",
      "Epoch 35\tReturn_mean: 279.06\tReturn_std: 117.43\tTime(Elapsed/Remain): 4.17/7.41 (mins)\n",
      "Epoch 40\tReturn_mean: 325.63\tReturn_std: 113.61\tTime(Elapsed/Remain): 6.04/8.69 (mins)\n",
      "Epoch 45\tReturn_mean: 399.72\tReturn_std: 122.64\tTime(Elapsed/Remain): 8.45/9.92 (mins)\n",
      "Epoch 50\tReturn_mean: 429.90\tReturn_std: 97.34\tTime(Elapsed/Remain): 11.10/10.66 (mins)\n",
      "Epoch 55\tReturn_mean: 441.48\tReturn_std: 86.38\tTime(Elapsed/Remain): 13.95/10.96 (mins)\n",
      "Epoch 60\tReturn_mean: 456.22\tReturn_std: 86.65\tTime(Elapsed/Remain): 16.85/10.78 (mins)\n",
      "Epoch 65\tReturn_mean: 474.78\tReturn_std: 66.90\tTime(Elapsed/Remain): 19.86/10.23 (mins)\n",
      "Epoch 70\tReturn_mean: 493.40\tReturn_std: 33.79\tTime(Elapsed/Remain): 22.88/9.35 (mins)\n",
      "Epoch 75\tReturn_mean: 483.57\tReturn_std: 57.95\tTime(Elapsed/Remain): 25.97/8.20 (mins)\n",
      "Epoch 80\tReturn_mean: 489.79\tReturn_std: 44.05\tTime(Elapsed/Remain): 29.04/6.81 (mins)\n",
      "Epoch 85\tReturn_mean: 479.35\tReturn_std: 59.41\tTime(Elapsed/Remain): 32.05/5.22 (mins)\n",
      "Epoch 90\tReturn_mean: 488.63\tReturn_std: 41.79\tTime(Elapsed/Remain): 35.10/3.47 (mins)\n",
      "Epoch 95\tReturn_mean: 493.34\tReturn_std: 34.31\tTime(Elapsed/Remain): 38.23/1.59 (mins)\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 100\n",
    "num_trajs = 100\n",
    "T = 10000\n",
    "log_interval = 5\n",
    "total_time = []\n",
    "\n",
    "pi = Policy(state_dim, num_action, 128, 128).to(device)\n",
    "optimizer_pi = optim.Adam(pi.parameters(), lr=1e-3)\n",
    "\n",
    "# For logging\n",
    "pi_returns, pi_returns_discounted = [], []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    start_epoch = time.time()\n",
    "    \n",
    "    # On-policy dataset\n",
    "    dataset = []\n",
    "    \n",
    "    # Collect trajectories to perform gradient step\n",
    "    for N in range(num_trajs):\n",
    "        data = {'log_pi':[], 'state':[], 'next_state':[], 'reward':[]}\n",
    "        r_sum, r_sum_discount = sample_trajectory(data, T)\n",
    "        dataset.append(data)\n",
    "\n",
    "        # For logging - store most recent N trajectories\n",
    "        pi_returns.append(r_sum)\n",
    "        pi_returns_discounted.append(r_sum_discount)\n",
    "        if len(pi_returns) > num_trajs:\n",
    "            pi_returns.pop(0)\n",
    "            pi_returns_discounted.pop(0)\n",
    "    \n",
    "    # Perform pocliy gradient step\n",
    "    optimizer_pi.zero_grad()\n",
    "    pi_loss = calculate_PG(pi_returns_discounted, dataset)\n",
    "    pi_loss.backward()\n",
    "    optimizer_pi.step()\n",
    "    \n",
    "    # Logging - print most recent epoch result\n",
    "    epoch_time = time.time() - start_epoch\n",
    "    total_time.append(epoch_time)\n",
    "    if epoch % log_interval == 0:\n",
    "        time_elapsed = np.sum(total_time)\n",
    "        time_remain = np.mean(total_time) * num_epochs - time_elapsed\n",
    "        print('Epoch {}\\tReturn_mean: {:.2f}\\tReturn_std: {:.2f}\\tTime(Elapsed/Remain): {:.2f}/{:.2f} (mins)'.format(\n",
    "            epoch, np.mean(pi_returns), np.std(pi_returns), time_elapsed/60, time_remain/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import linalg as LA\n",
    "\n",
    "# Calculate feature vector\n",
    "#state[0] : Cart pos\n",
    "#state[1] : Cart speed\n",
    "#state[2] : Pole angle\n",
    "#state[3] : Pole velocity at tip\n",
    "\n",
    "state2 = [-0.12, 0, 0.12] # termination condition\n",
    "state3 = [-1, 0, 1]\n",
    "\n",
    "mu = []\n",
    "for s2 in state2:\n",
    "    for s3 in state3:\n",
    "        mu.append([s2, s3])\n",
    "\n",
    "def state2feature(state):\n",
    "    phi = []\n",
    "    for f in mu:\n",
    "        rad_base = LA.norm(np.array(state[-2:])-np.array(f)) ** 2\n",
    "        phi.append(np.exp(-0.5*rad_base))\n",
    "    return np.array(phi)\n",
    "\n",
    "\n",
    "def calculate_vf(dataset, vf):\n",
    "    X, y = [], []\n",
    "    \n",
    "    for data in dataset:\n",
    "        for s, next_s, r in zip(data['state'], data['next_state'], data['reward']):\n",
    "            v = state2feature(s)\n",
    "            Q = r\n",
    "            if vf is not None:\n",
    "                Q = r + gamma * vf.predict(state2feature(next_s).reshape(1, -1))[0]\n",
    "            X.append(v)\n",
    "            y.append(Q)\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "\n",
    "def get_advantage(data, vf):\n",
    "    advantage, baseline = [], []\n",
    "    \n",
    "    for s, next_s, r in zip(data['state'], data['next_state'], data['reward']):\n",
    "        v = vf.predict(state2feature(s).reshape(1, -1))[0]\n",
    "        v_next = vf.predict(state2feature(next_s).reshape(1, -1))[0]\n",
    "        # TODO: Complete advantage calculation by calculating Q-value\n",
    "        #Q = \n",
    "        A = Q - v\n",
    "        \n",
    "        advantage.append(A)\n",
    "        baseline.append(v)\n",
    "    \n",
    "    return advantage, baseline\n",
    "\n",
    "\n",
    "def calculate_AC_PG(vf, pi_returns_discounted, dataset):\n",
    "    pi_loss = 0\n",
    "    for data in dataset:\n",
    "        # For linear Actor-Critic\n",
    "        advantage = []\n",
    "        _, v = get_advantage(data, vf)\n",
    "        DCR = 0\n",
    "        for i, r in enumerate(reversed(data['reward'])):\n",
    "            DCR = r + gamma * DCR\n",
    "            advantage.insert(0, DCR - v[i]) # For practical algorithm, we just adopt baseline\n",
    "\n",
    "        # Compute each element of gradient\n",
    "        pi_loss_linear_vf = [log_pi * a for log_pi, a in zip(data['log_pi'], advantage)]\n",
    "        \n",
    "        # Sums up log_prob * weight\n",
    "        pi_loss += torch.cat(pi_loss_linear_vf).sum()\n",
    "        \n",
    "    return pi_loss / num_trajs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 100\n",
    "num_trajs = 100\n",
    "T = 10000\n",
    "log_interval = 5\n",
    "total_time = []\n",
    "\n",
    "pi = Policy(state_dim, num_action, 128, 128).to(device)\n",
    "optimizer_pi = optim.Adam(pi.parameters(), lr=1e-3)\n",
    "vf = None\n",
    "\n",
    "# For logging\n",
    "pi_returns, pi_returns_discounted = [], []\n",
    "\n",
    "dataset_vf = []\n",
    "for epoch in range(num_epochs):\n",
    "    start_epoch = time.time()\n",
    "    \n",
    "    # On-policy dataset\n",
    "    dataset = []\n",
    "    \n",
    "    # Collect trajectories to perform gradient step\n",
    "    for N in range(num_trajs):\n",
    "        data = {'log_pi':[], 'state':[], 'next_state':[], 'reward':[]}\n",
    "        r_sum, r_sum_discount = sample_trajectory(data, T)\n",
    "        dataset.append(data)\n",
    "        dataset_vf.append(data)\n",
    "\n",
    "        # For logging - store most recent N trajectories\n",
    "        pi_returns.append(r_sum)\n",
    "        pi_returns_discounted.append(r_sum_discount)\n",
    "        if len(pi_returns) > num_trajs:\n",
    "            pi_returns.pop(0)\n",
    "            pi_returns_discounted.pop(0)\n",
    "\n",
    "    ### NEW : update critic ###\n",
    "    X, y = calculate_vf(dataset_vf, vf)\n",
    "    vf = LinearRegression().fit(X, y)\n",
    "    \n",
    "    # Perform pocliy gradient step\n",
    "    optimizer_pi.zero_grad()\n",
    "    pi_loss = calculate_AC_PG(vf, pi_returns_discounted, dataset)\n",
    "    pi_loss.backward()\n",
    "    optimizer_pi.step()\n",
    "    \n",
    "    # Logging - print most recent epoch result\n",
    "    epoch_time = time.time() - start_epoch\n",
    "    total_time.append(epoch_time)\n",
    "    if epoch % log_interval == 0:\n",
    "        dataset_vf = []\n",
    "        time_elapsed = np.sum(total_time)\n",
    "        time_remain = np.mean(total_time) * num_epochs - time_elapsed\n",
    "        print('Epoch {}\\tReturn_mean: {:.2f}\\tReturn_std: {:.2f}\\tTime(Elapsed/Remain): {:.2f}/{:.2f} (mins)'.format(\n",
    "            epoch, np.mean(pi_returns), np.std(pi_returns), time_elapsed/60, time_remain/60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For rendering **[COLAB USE ONLY!]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym.wrappers import Monitor\n",
    "import glob\n",
    "import io\n",
    "import base64\n",
    "from IPython.display import HTML\n",
    "from pyvirtualdisplay import Display\n",
    "from IPython import display as ipythondisplay\n",
    "\n",
    "def show_video():\n",
    "    mp4list = glob.glob('video/*.mp4')\n",
    "    if len(mp4list) > 0:\n",
    "        mp4 = mp4list[0]\n",
    "        video = io.open(mp4, 'r+b').read()\n",
    "        encoded = base64.b64encode(video)\n",
    "        ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n",
    "            loop controls style=\"height: 400px;\">\n",
    "            <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
    "            </video>'''.format(encoded.decode('ascii'))))\n",
    "    else:\n",
    "        print(\"Could not find video\")\n",
    "    \n",
    "def wrap_env(env):\n",
    "    env = Monitor(env, './video', force=True)\n",
    "    return env\n",
    "\n",
    "if colab:\n",
    "    display = Display(visible=0, size=(1400, 900))\n",
    "    display.start()\n",
    "\n",
    "    env = wrap_env(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = env.reset()\n",
    "\n",
    "while True:\n",
    "    env.render()\n",
    "    time.sleep(0.01)\n",
    "    action, log_prob = select_action(state)\n",
    "    state, reward, done, info = env.step(action)\n",
    "    if done: \n",
    "        break\n",
    "            \n",
    "env.close()\n",
    "if colab:\n",
    "    show_video()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
