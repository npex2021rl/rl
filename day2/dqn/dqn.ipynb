{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q-network Practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "%cd /content/drive/MyDrive/Colab\\ Notebooks/rl-master/day1\n",
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -1. Introduction to Gym environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## -1.1 Prerequisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
    "!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1\n",
    "!apt-get update > /dev/null 2>&1\n",
    "!apt-get install cmake > /dev/null 2>&1\n",
    "!pip install --upgrade setuptools 2>&1\n",
    "!pip install ez_setup > /dev/null 2>&1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym.wrappers import Monitor\n",
    "import glob\n",
    "import io\n",
    "import base64\n",
    "from IPython.display import HTML\n",
    "from pyvirtualdisplay import Display\n",
    "from IPython import display as ipythondisplay\n",
    "\n",
    "display = Display(visible=0, size=(1400, 900))\n",
    "display.start()\n",
    "\n",
    "\"\"\"\n",
    "Utility functions to enable video recording of gym environment \n",
    "and displaying it.\n",
    "To enable video, just do \"env = wrap_env(env)\"\"\n",
    "\"\"\"\n",
    "def show_video():\n",
    "    mp4list = glob.glob('video/*.mp4')\n",
    "    if len(mp4list) > 0:\n",
    "        mp4 = mp4list[0]\n",
    "        video = io.open(mp4, 'r+b').read()\n",
    "        encoded = base64.b64encode(video)\n",
    "        ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n",
    "                loop controls style=\"height: 400px;\">\n",
    "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
    "             </video>'''.format(encoded.decode('ascii'))))\n",
    "    else: \n",
    "        print(\"Could not find video\")\n",
    "    \n",
    "\n",
    "def wrap_env(env):\n",
    "    env = Monitor(env, './video', force=True)\n",
    "    return env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Define Q-network & policy-network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import MSELoss\n",
    "import torch.nn.functional as F\n",
    "import copy\n",
    "import os\n",
    "import csv\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.optim import Adam\n",
    "from buffer import ReplayBuffer\n",
    "from utils import save_snapshot, recover_snapshot, load_model\n",
    "from schedule import LinearSchedule\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current device = cuda\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('current device =', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# critic network definition\n",
    "# multi-layer perceptron (with 2 hidden layers)\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, state_dim, num_action, hidden_size1, hidden_size2):\n",
    "        super(Critic, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim, hidden_size1)\n",
    "        self.fc2 = nn.Linear(hidden_size1, hidden_size2)\n",
    "        self.fc3 = nn.Linear(hidden_size2, num_action)\n",
    "\n",
    "\n",
    "    def forward(self, state):\n",
    "        # given a state s, the network returns a vector Q(s,) of length |A|\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        q = self.fc3(x)\n",
    "        return q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Define DDPG agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self, obs_dim, num_act, hidden1, hidden2):\n",
    "        self.obs_dim = obs_dim\n",
    "        self.num_act = num_act\n",
    "        # networks\n",
    "        self.critic = Critic(obs_dim, num_act, hidden1, hidden2).to(device)\n",
    "                \n",
    "    def act(self, state, epsilon=0.0):\n",
    "        # simple implementation of \\epsilon-greedy method\n",
    "        if np.random.rand() < epsilon:\n",
    "            return np.random.randint(self.num_act)\n",
    "        else:\n",
    "            # greedy selection\n",
    "            self.critic.eval()\n",
    "            s = torch.Tensor(state).view(1, self.obs_dim).to(device)\n",
    "            q = self.critic(s)\n",
    "            return np.argmax(q.cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Implement one-step param update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update(agent, replay_buf, gamma, critic_optim, target_critic, tau, batch_size):\n",
    "    # agent : agent with networks to be trained\n",
    "    # replay_buf : replay buf from which we sample a batch\n",
    "    # actor_optim / critic_optim : torch optimizers\n",
    "    # tau : parameter for soft target update\n",
    "    \n",
    "    agent.critic.train()\n",
    "\n",
    "    batch = replay_buf.sample_batch(batch_size)\n",
    "\n",
    "    # unroll batch\n",
    "    with torch.no_grad():\n",
    "        observations = torch.Tensor(batch['state']).to(device)\n",
    "        actions = torch.tensor(batch['action'], dtype=torch.long).to(device)\n",
    "        rewards = torch.Tensor(batch['reward']).to(device)\n",
    "        next_observations = torch.Tensor(batch['next_state']).to(device)\n",
    "        terminals = torch.Tensor(batch['done']).to(device)\n",
    "\n",
    "        mask = 1.0 - terminals\n",
    "\n",
    "        next_q = torch.unsqueeze(target_critic(next_observations).max(1)[0], 1)\n",
    "        target = rewards + gamma * mask * next_q\n",
    "\n",
    "    out = agent.critic(observations).gather(1, actions)\n",
    "\n",
    "    loss_ftn = MSELoss()\n",
    "    loss = loss_ftn(out, target)\n",
    "\n",
    "    critic_optim.zero_grad()\n",
    "    loss.backward()\n",
    "    critic_optim.step()\n",
    "        \n",
    "    # soft target update (both actor & critic network)\n",
    "    for p, targ_p in zip(agent.critic.parameters(), target_critic.parameters()):\n",
    "        targ_p.data.copy_((1. - tau) * targ_p + tau * p)\n",
    "        \n",
    "        \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(agent, env, num_episodes=5):\n",
    "\n",
    "    sum_scores = 0.\n",
    "    \n",
    "    for i in range(num_episodes):\n",
    "        obs = env.reset()\n",
    "        done = False\n",
    "        score = 0.\n",
    "        \n",
    "        while not done:\n",
    "            action = agent.act(obs)\n",
    "            obs, rew, done, _ = env.step(action)\n",
    "            score += rew\n",
    "        sum_scores += score\n",
    "    avg_score = sum_scores / num_episodes\n",
    "    \n",
    "    return avg_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Combining these, we finally have..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(agent, env, gamma, \n",
    "          lr, tau,\n",
    "          ep_len, num_updates, batch_size,\n",
    "          init_buffer=5000, buffer_size=100000,\n",
    "          start_train=2000, train_interval=50,\n",
    "          eval_interval=2000, snapshot_interval=10000,\n",
    "          path=None):\n",
    "    \n",
    "    target_critic = copy.deepcopy(agent.critic)\n",
    "    \n",
    "    # environment for evaluation\n",
    "    test_env = copy.deepcopy(env)\n",
    "    # freeze target network\n",
    "    for p in target_critic.parameters():\n",
    "        p.requires_grad_(False)\n",
    "\n",
    "    critic_optim = Adam(agent.critic.parameters(), lr=lr)\n",
    "\n",
    "    if path is not None:\n",
    "        recover_snapshot(path, agent.critic,\n",
    "                         target_critic, critic_optim,\n",
    "                         device=device\n",
    "                        )\n",
    "        # load snapshot\n",
    "    \n",
    "    obs_dim = env.observation_space.shape[0]\n",
    "    num_act = env.action_space.n\n",
    "    \n",
    "    replay_buf = ReplayBuffer(obs_dim, buffer_size)\n",
    "    \n",
    "    max_epsilon = 1.\n",
    "    min_epsilon = 0.02\n",
    "    exploration_schedule = LinearSchedule(begin_t=start_train,\n",
    "                                          end_t=num_updates,\n",
    "                                          begin_value=max_epsilon,\n",
    "                                          end_value=min_epsilon\n",
    "                                         )\n",
    "    save_path = './snapshots/'\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    os.makedirs('./learning_curves/', exist_ok=True)\n",
    "    log_file = open('./learning_curves/res.csv',\n",
    "                    'w',\n",
    "                    encoding='utf-8',\n",
    "                    newline=''\n",
    "                   )\n",
    "    logger = csv.writer(log_file)\n",
    "    \n",
    "    # main loop\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    step_count = 0\n",
    "    \n",
    "    for t in range(num_updates + 1):\n",
    "        if t < init_buffer:\n",
    "            # perform random action until we collect sufficiently many samples\n",
    "            # this is for exploration purpose\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            # executes epsilon-greedy action\n",
    "            epsilon = exploration_schedule(t)\n",
    "            action = agent.act(obs, epsilon=epsilon)\n",
    "            \n",
    "        next_obs, rew, done, _ = env.step(action)\n",
    "        step_count += 1\n",
    "        if step_count == ep_len:\n",
    "            # if the next_state is not terminal but done is set to True by gym env wrapper\n",
    "            done = False\n",
    "            \n",
    "        replay_buf.append(obs, action, next_obs, rew, done)\n",
    "        obs = next_obs\n",
    "        \n",
    "        if done == True or step_count == ep_len:\n",
    "            # reset environment if current environment reaches a terminal state \n",
    "            # or step count reaches predefined length\n",
    "            obs = env.reset()\n",
    "            done = False\n",
    "            step_count = 0\n",
    "            # score = evaluate(agent, env)\n",
    "            # print('[iteration {}] evaluation score : {}'.format(t, score))\n",
    "        \n",
    "        if t % eval_interval == 0:\n",
    "            avg_score = evaluate(agent, test_env, num_episodes=5)\n",
    "            print('[iter {}] average score = {} (over 5 episodes)'.format(t, avg_score))\n",
    "            evaluation_log = [t, avg_score]\n",
    "            logger.writerow(evaluation_log)\n",
    "        \n",
    "        if t % snapshot_interval == 0:\n",
    "            snapshot_path = save_path + 'iter{}_'.format(t)\n",
    "            # save weight & training progress\n",
    "            save_snapshot(snapshot_path, agent.critic, target_critic, critic_optim)\n",
    "        \n",
    "        if t > start_train and t % train_interval == 0:\n",
    "            # start training after fixed number of steps\n",
    "            # this may mitigate overfitting of networks to the \n",
    "            # small number of samples collected during the initial stage of training\n",
    "            for _ in range(train_interval):\n",
    "                update(agent,\n",
    "                       replay_buf,\n",
    "                       gamma,\n",
    "                       critic_optim,\n",
    "                       target_critic,\n",
    "                       tau,\n",
    "                       batch_size\n",
    "                      )\n",
    "\n",
    "    log_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Let's train our agent!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "observation space dim. : 4 / # actions : 2\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "obs_dim = env.observation_space.shape[0]\n",
    "num_act = env.action_space.n\n",
    "\n",
    "print('observation space dim. : {} / # actions : {}'.format(obs_dim, num_act))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = DQNAgent(obs_dim=obs_dim, num_act=num_act, hidden1=256, hidden2=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.99\n",
    "lr = 1e-3\n",
    "tau = 1e-3\n",
    "ep_len = 500\n",
    "num_updates = 100000\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train(agent, env, gamma, \n",
    "      lr, tau,\n",
    "      ep_len, num_updates, batch_size,\n",
    "      init_buffer=5000, buffer_size=100000,\n",
    "      start_train=2000, train_interval=50,\n",
    "      eval_interval=2000, snapshot_interval=2000, path=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Watch the trained agent!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading pre-trained weight...\n",
      "score :  500.0\n"
     ]
    }
   ],
   "source": [
    "# env = wrap_env(env)\n",
    "obs = env.reset()\n",
    "done = False\n",
    "score = 0.\n",
    "load_model(agent, path='./snapshots/trained.pth.tar', device=device)\n",
    "while not done:\n",
    "    env.render()\n",
    "    obs, rew, done, _ = env.step(agent.act(obs))\n",
    "    score += rew\n",
    "    \n",
    "env.close()\n",
    "print('score : ', score)\n",
    "# show_video()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
