{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q-network Practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "%cd /content/drive/MyDrive/Colab\\ Notebooks/rl-master/day1\n",
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -1. Introduction to Gym environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## -1.1 Prerequisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting setuptools\n",
      "  Downloading setuptools-57.4.0-py3-none-any.whl (819 kB)\n",
      "\u001b[K     |████████████████████████████████| 819 kB 1.4 MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: setuptools\n",
      "  Attempting uninstall: setuptools\n",
      "    Found existing installation: setuptools 45.2.0.post20200210\n",
      "    Uninstalling setuptools-45.2.0.post20200210:\n",
      "      Successfully uninstalled setuptools-45.2.0.post20200210\n",
      "Successfully installed setuptools-57.4.0\n"
     ]
    }
   ],
   "source": [
    "!pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
    "!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1\n",
    "!apt-get update > /dev/null 2>&1\n",
    "!apt-get install cmake > /dev/null 2>&1\n",
    "!pip install --upgrade setuptools 2>&1\n",
    "!pip install ez_setup > /dev/null 2>&1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym.wrappers import Monitor\n",
    "import glob\n",
    "import io\n",
    "import base64\n",
    "from IPython.display import HTML\n",
    "from pyvirtualdisplay import Display\n",
    "from IPython import display as ipythondisplay\n",
    "\n",
    "display = Display(visible=0, size=(1400, 900))\n",
    "display.start()\n",
    "\n",
    "\"\"\"\n",
    "Utility functions to enable video recording of gym environment \n",
    "and displaying it.\n",
    "To enable video, just do \"env = wrap_env(env)\"\"\n",
    "\"\"\"\n",
    "def show_video():\n",
    "  mp4list = glob.glob('video/*.mp4')\n",
    "  if len(mp4list) > 0:\n",
    "    mp4 = mp4list[0]\n",
    "    video = io.open(mp4, 'r+b').read()\n",
    "    encoded = base64.b64encode(video)\n",
    "    ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n",
    "                loop controls style=\"height: 400px;\">\n",
    "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
    "             </video>'''.format(encoded.decode('ascii'))))\n",
    "  else: \n",
    "    print(\"Could not find video\")\n",
    "    \n",
    "\n",
    "def wrap_env(env):\n",
    "  env = Monitor(env, './video', force=True)\n",
    "  return env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Define Q-network & policy-network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import MSELoss\n",
    "import torch.nn.functional as F\n",
    "import copy\n",
    "import os\n",
    "import csv\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.optim import Adam\n",
    "from buffer import ReplayBuffer\n",
    "from utils import save_snapshot, recover_snapshot, load_model\n",
    "from schedule import LinearSchedule\n",
    "import gym\n",
    "import pybulletgym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current device = cuda\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('current device =', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# critic network definition\n",
    "# multi-layer perceptron (with 2 hidden layers)\n",
    "class Critic(nn.Module):\n",
    "    \"\"\"\n",
    "    implementation of critic network Q(s, a)\n",
    "    \"\"\"\n",
    "    def __init__(self, state_dim, num_action, hidden_size1, hidden_size2):\n",
    "        super(Critic, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim, hidden_size1)\n",
    "        self.fc2 = nn.Linear(hidden_size1, hidden_size2)\n",
    "        self.fc3 = nn.Linear(hidden_size2, num_action)\n",
    "\n",
    "\n",
    "    def forward(self, state):\n",
    "        # given a state s, the network returns a vector Q(s,) of length |A|\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        q = self.fc3(x)\n",
    "\n",
    "        return q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Define DQN agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self, obs_dim, num_act, hidden1, hidden2):\n",
    "        self.obs_dim = obs_dim\n",
    "        self.num_act = num_act\n",
    "        # networks\n",
    "        self.critic = Critic(obs_dim, num_act, hidden1, hidden2).to(device)\n",
    "                \n",
    "    def act(self, state, epsilon=0.0):\n",
    "        # simple implementation of \\epsilon-greedy method\n",
    "        if np.random.rand() < epsilon:\n",
    "            # TODO: Write down epsilon-greedy rule. \n",
    "            # Hint : numpy.random.randint(n) returns a random number drawn from 0, ..., n - 1\n",
    "            # a = ...\n",
    "            # return a\n",
    "        else:\n",
    "            # greedy selection\n",
    "            self.critic.eval()\n",
    "            s = torch.Tensor(state).view(1, self.obs_dim).to(device)\n",
    "            q = self.critic(s)\n",
    "            a = np.argmax(q.cpu().detach().numpy())\n",
    "            return a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Implement one-step param update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update(agent, replay_buf, gamma, critic_optim, target_critic, tau, batch_size):\n",
    "    # agent : agent with networks to be trained\n",
    "    # replay_buf : replay buf from which we sample a batch\n",
    "    # actor_optim / critic_optim : torch optimizers\n",
    "    # tau : parameter for soft target update\n",
    "    \n",
    "    agent.critic.train()\n",
    "\n",
    "    batch = replay_buf.sample_batch(batch_size)\n",
    "\n",
    "    # unroll batch\n",
    "    with torch.no_grad():\n",
    "        observations = torch.Tensor(batch['state']).to(device)\n",
    "        actions = torch.tensor(batch['action'], dtype=torch.long).to(device)\n",
    "        rewards = torch.Tensor(batch['reward']).to(device)\n",
    "        next_observations = torch.Tensor(batch['next_state']).to(device)\n",
    "        terminals = torch.Tensor(batch['done']).to(device)\n",
    "\n",
    "        mask = 1.0 - terminals\n",
    "\n",
    "        next_q = mask * torch.unsqueeze(target_critic(next_observations).max(1)[0], 1)\n",
    "        # TODO : Fill the below line\n",
    "        # target = ...\n",
    "\n",
    "    out = agent.critic(observations).gather(1, actions)\n",
    "\n",
    "    loss_ftn = MSELoss()\n",
    "    loss = loss_ftn(out, target)\n",
    "\n",
    "    critic_optim.zero_grad()\n",
    "    loss.backward()\n",
    "    critic_optim.step()\n",
    "        \n",
    "    # soft target update (both actor & critic network)\n",
    "    for p, targ_p in zip(agent.critic.parameters(), target_critic.parameters()):\n",
    "        targ_p.data.copy_((1. - tau) * targ_p + tau * p)\n",
    "        \n",
    "        \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(agent, env, num_episodes=5):\n",
    "\n",
    "    sum_scores = 0.\n",
    "    \n",
    "    for i in range(num_episodes):\n",
    "        obs = env.reset()\n",
    "        done = False\n",
    "        score = 0.\n",
    "        \n",
    "        while not done:\n",
    "            action = agent.act(obs)\n",
    "            obs, rew, done, _ = env.step(action)\n",
    "            score += rew\n",
    "        sum_scores += score\n",
    "    avg_score = sum_scores / num_episodes\n",
    "    \n",
    "    return avg_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Combining these, we finally have..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(agent, env, gamma, \n",
    "          lr, tau,\n",
    "          ep_len, num_updates, batch_size,\n",
    "          init_buffer=5000, buffer_size=100000,\n",
    "          start_train=2000, train_interval=50,\n",
    "          eval_interval=2000, snapshot_interval=10000,\n",
    "          path=None):\n",
    "    \n",
    "    target_critic = copy.deepcopy(agent.critic)\n",
    "    \n",
    "    # environment for evaluation\n",
    "    test_env = copy.deepcopy(env)\n",
    "    # freeze target network\n",
    "    for p in target_critic.parameters():\n",
    "        p.requires_grad_(False)\n",
    "\n",
    "    critic_optim = Adam(agent.critic.parameters(), lr=lr)\n",
    "\n",
    "    if path is not None:\n",
    "        recover_snapshot(path, agent.critic,\n",
    "                         target_critic, critic_optim,\n",
    "                         device=device\n",
    "                        )\n",
    "        # load snapshot\n",
    "    \n",
    "    obs_dim = env.observation_space.shape[0]\n",
    "    num_act = env.action_space.n\n",
    "    \n",
    "    replay_buf = ReplayBuffer(obs_dim, buffer_size)\n",
    "    \n",
    "    max_epsilon = 1.\n",
    "    min_epsilon = 0.02\n",
    "    exploration_schedule = LinearSchedule(begin_t=start_train,\n",
    "                                          end_t=num_updates,\n",
    "                                          begin_value=max_epsilon,\n",
    "                                          end_value=min_epsilon\n",
    "                                         )\n",
    "    save_path = './snapshots/'\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    os.makedirs('./learning_curves/', exist_ok=True)\n",
    "    log_file = open('./learning_curves/res.csv',\n",
    "                    'w',\n",
    "                    encoding='utf-8',\n",
    "                    newline=''\n",
    "                   )\n",
    "    logger = csv.writer(log_file)\n",
    "    \n",
    "    # main loop\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    step_count = 0\n",
    "    \n",
    "    for t in range(num_updates + 1):\n",
    "        if t < init_buffer:\n",
    "            # perform random action until we collect sufficiently many samples\n",
    "            # this is for exploration purpose\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            # executes epsilon-greedy action\n",
    "            epsilon = exploration_schedule(t)\n",
    "            action = agent.act(obs, epsilon=epsilon)\n",
    "            \n",
    "        next_obs, rew, done, _ = env.step(action)\n",
    "        step_count += 1\n",
    "        if step_count == ep_len:\n",
    "            # if the next_state is not terminal but done is set to True by gym env wrapper\n",
    "            done = False\n",
    "            \n",
    "        replay_buf.append(obs, action, next_obs, rew, done)\n",
    "        obs = next_obs\n",
    "        \n",
    "        if done == True or step_count == ep_len:\n",
    "            # reset environment if current environment reaches a terminal state \n",
    "            # or step count reaches predefined length\n",
    "            obs = env.reset()\n",
    "            done = False\n",
    "            step_count = 0\n",
    "            # score = evaluate(agent, env)\n",
    "            # print('[iteration {}] evaluation score : {}'.format(t, score))\n",
    "        \n",
    "        if t % eval_interval == 0:\n",
    "            avg_score = evaluate(agent, test_env, num_episodes=5)\n",
    "            print('[iter {}] average score = {} (over 5 episodes)'.format(t, avg_score))\n",
    "            evaluation_log = [t, avg_score]\n",
    "            logger.writerow(evaluation_log)\n",
    "        \n",
    "        if t % snapshot_interval == 0:\n",
    "            snapshot_path = save_path + 'iter{}_'.format(t)\n",
    "            # save weight & training progress\n",
    "            save_snapshot(snapshot_path, agent.critic, target_critic, critic_optim)\n",
    "        \n",
    "        if t > start_train and t % train_interval == 0:\n",
    "            # start training after fixed number of steps\n",
    "            # this may mitigate overfitting of networks to the \n",
    "            # small number of samples collected during the initial stage of training\n",
    "            for _ in range(train_interval):\n",
    "                update(agent,\n",
    "                       replay_buf,\n",
    "                       gamma,\n",
    "                       critic_optim,\n",
    "                       target_critic,\n",
    "                       tau,\n",
    "                       batch_size\n",
    "                      )\n",
    "\n",
    "    log_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Let's train our agent!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "observation space dim. : 4 / # actions : 2\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "obs_dim = env.observation_space.shape[0]\n",
    "num_act = env.action_space.n\n",
    "\n",
    "print('observation space dim. : {} / # actions : {}'.format(obs_dim, num_act))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = DQNAgent(obs_dim=obs_dim, num_act=num_act, hidden1=256, hidden2=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO : Feel free to try other parameters!\n",
    "gamma = 0.99\n",
    "lr = 1e-3\n",
    "tau = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ep_len = 500\n",
    "num_updates = 100000\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[iter 0] average score = 9.0 (over 5 episodes)\n",
      "[iter 2000] average score = 9.2 (over 5 episodes)\n",
      "[iter 4000] average score = 17.0 (over 5 episodes)\n",
      "[iter 6000] average score = 94.8 (over 5 episodes)\n",
      "[iter 8000] average score = 62.4 (over 5 episodes)\n",
      "[iter 10000] average score = 99.2 (over 5 episodes)\n",
      "[iter 12000] average score = 89.0 (over 5 episodes)\n",
      "[iter 14000] average score = 80.4 (over 5 episodes)\n",
      "[iter 16000] average score = 80.8 (over 5 episodes)\n",
      "[iter 18000] average score = 91.6 (over 5 episodes)\n",
      "[iter 20000] average score = 113.6 (over 5 episodes)\n",
      "[iter 22000] average score = 155.0 (over 5 episodes)\n",
      "[iter 24000] average score = 90.2 (over 5 episodes)\n",
      "[iter 26000] average score = 88.0 (over 5 episodes)\n",
      "[iter 28000] average score = 93.0 (over 5 episodes)\n",
      "[iter 30000] average score = 90.4 (over 5 episodes)\n",
      "[iter 32000] average score = 115.6 (over 5 episodes)\n",
      "[iter 34000] average score = 112.6 (over 5 episodes)\n",
      "[iter 36000] average score = 117.4 (over 5 episodes)\n",
      "[iter 38000] average score = 89.0 (over 5 episodes)\n",
      "[iter 40000] average score = 90.4 (over 5 episodes)\n",
      "[iter 42000] average score = 186.2 (over 5 episodes)\n",
      "[iter 44000] average score = 90.6 (over 5 episodes)\n",
      "[iter 46000] average score = 124.6 (over 5 episodes)\n",
      "[iter 48000] average score = 132.0 (over 5 episodes)\n",
      "[iter 50000] average score = 117.6 (over 5 episodes)\n",
      "[iter 52000] average score = 188.2 (over 5 episodes)\n",
      "[iter 54000] average score = 176.8 (over 5 episodes)\n",
      "[iter 56000] average score = 198.2 (over 5 episodes)\n",
      "[iter 58000] average score = 194.4 (over 5 episodes)\n",
      "[iter 60000] average score = 200.0 (over 5 episodes)\n",
      "[iter 62000] average score = 200.0 (over 5 episodes)\n",
      "[iter 64000] average score = 187.6 (over 5 episodes)\n",
      "[iter 66000] average score = 200.0 (over 5 episodes)\n",
      "[iter 68000] average score = 200.0 (over 5 episodes)\n",
      "[iter 70000] average score = 183.6 (over 5 episodes)\n",
      "[iter 72000] average score = 200.0 (over 5 episodes)\n",
      "[iter 74000] average score = 200.0 (over 5 episodes)\n",
      "[iter 76000] average score = 166.0 (over 5 episodes)\n",
      "[iter 78000] average score = 200.0 (over 5 episodes)\n",
      "[iter 80000] average score = 200.0 (over 5 episodes)\n",
      "[iter 82000] average score = 150.2 (over 5 episodes)\n",
      "[iter 84000] average score = 200.0 (over 5 episodes)\n",
      "[iter 86000] average score = 128.2 (over 5 episodes)\n",
      "[iter 88000] average score = 168.6 (over 5 episodes)\n",
      "[iter 90000] average score = 182.6 (over 5 episodes)\n",
      "[iter 92000] average score = 141.2 (over 5 episodes)\n",
      "[iter 94000] average score = 158.2 (over 5 episodes)\n",
      "[iter 96000] average score = 100.4 (over 5 episodes)\n",
      "[iter 98000] average score = 133.8 (over 5 episodes)\n",
      "[iter 100000] average score = 83.2 (over 5 episodes)\n"
     ]
    }
   ],
   "source": [
    "train(agent, env, gamma, \n",
    "      lr, tau,\n",
    "      ep_len, num_updates, batch_size,\n",
    "      init_buffer=5000, buffer_size=100000,\n",
    "      start_train=2000, train_interval=50,\n",
    "      eval_interval=2000, snapshot_interval=10000, path=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Watch the trained agent!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading pre-trained weight...\n",
      "score :  200.0\n"
     ]
    }
   ],
   "source": [
    "env = wrap_env(env)\n",
    "obs = env.reset()\n",
    "done = False\n",
    "score = 0.\n",
    "load_model(agent, path='./snapshots/trained.pth.tar', device=device)\n",
    "while not done:\n",
    "    env.render()\n",
    "    obs, rew, done, _ = env.step(agent.act(obs))\n",
    "    score += rew\n",
    "    \n",
    "env.close()\n",
    "print('score : ', score)\n",
    "show_video()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
