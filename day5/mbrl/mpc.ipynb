{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import truncnorm\n",
    "import warnings\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from torch.nn import MSELoss\n",
    "from my_pendulum import MyPendulumEnv\n",
    "from reward_ftns import pendulum_reward\n",
    "from memory import TransitionMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransitionModel(nn.Module):\n",
    "    def __init__(self, state_dim, act_dim, hidden1, hidden2):\n",
    "        super(TransitionModel, self).__init__()\n",
    "        self.state_dim = state_dim\n",
    "        self.act_dim = act_dim\n",
    "        self.fc1 = nn.Linear(state_dim + act_dim, hidden1)\n",
    "        self.fc2 = nn.Linear(hidden1, hidden2)\n",
    "        self.fc3 = nn.Linear(hidden2, state_dim)\n",
    "\n",
    "    def forward(self, state, act):\n",
    "        x = torch.cat([state, act], dim=1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        delta = self.fc3(x)\n",
    "\n",
    "        next_state = state + delta  # \\hat{s}_{t+1} = s_t + f(s_t, a_t ; \\theta)\n",
    "        \n",
    "        return next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelBasedAgent:\n",
    "    def __init__(self,\n",
    "                 state_dim,\n",
    "                 act_dim,\n",
    "                 ctrl_range,\n",
    "                 reward_ftn,\n",
    "                 hidden1=400,\n",
    "                 hidden2=400,\n",
    "                 lr=0.001,\n",
    "                 mem_sz=20000\n",
    "                 ):\n",
    "        self.dimS = state_dim\n",
    "        self.dimA = act_dim\n",
    "        self.ctrl_range = ctrl_range\n",
    "        self.model = TransitionModel(state_dim, act_dim, hidden1, hidden2)\n",
    "        self.memory = TransitionMemory(state_dim, act_dim, maxlen=mem_sz)\n",
    "        \n",
    "        self.reward_model = reward_ftn\n",
    "        \n",
    "        self.optimizer = Adam(self.model.parameters(), lr=lr)\n",
    "        \n",
    "        self.alpha = 0.5    # smoothing parameter of cross-entropy optimization\n",
    "        self.Ne = 20        # number of elite samples\n",
    "\n",
    "    def train(self, batch_size):\n",
    "        self.model.train()\n",
    "        # note that training of the dynamics does not depend on any reward info\n",
    "        (state_batch, act_batch, next_state_batch) = self.memory.sample_batch(batch_size)\n",
    "\n",
    "        state_batch = torch.tensor(state_batch).float()\n",
    "        act_batch = torch.tensor(act_batch).float()\n",
    "        next_state_batch = torch.tensor(next_state_batch).float()\n",
    "\n",
    "        prediction = self.model(state_batch, act_batch)\n",
    "\n",
    "\n",
    "        loss_ftn = MSELoss()\n",
    "        loss = loss_ftn(prediction, next_state_batch)\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        loss_val = loss.detach().numpy()\n",
    "\n",
    "        return loss_val\n",
    "\n",
    "    def act(self, state, H, N, cross_entropy=False):\n",
    "        if cross_entropy:\n",
    "            return self.solve_cross_entropy_opt(state, H, N)\n",
    "        else:\n",
    "            return self.solve_random_shooting_opt(state, H, N)\n",
    "    \n",
    "    \n",
    "    def solve_random_shooting_opt(self, state, H, N):\n",
    "        \"\"\"\n",
    "        # generate K trajectories using the model of dynamics and random action sampling, and perform MPC\n",
    "        # Remark! K roll-outs can be done simultaneously!\n",
    "\n",
    "        given a state, execute an action based on random-sampling shooting method\n",
    "        :param state: current state(numpy array)\n",
    "        :param rew_ftn: vectorized reward function\n",
    "        :param K: number of candidate action sequences to generate\n",
    "        :param H: length of time horizon\n",
    "        :return: action to be executed(numpy array)\n",
    "        \"\"\"\n",
    "        assert N > 0 and H > 0\n",
    "\n",
    "        dimA = self.dimA\n",
    "\n",
    "        self.model.eval()\n",
    "\n",
    "        states = np.tile(state, (N, 1)) # shape = (K, dim S)\n",
    "        scores = np.zeros(N)    # array which contains cumulative rewards of roll-outs\n",
    "\n",
    "        # generate K random action sequences of length H\n",
    "        action_sequences = self.ctrl_range * (2. * np.random.rand(H, N, dimA) - 1.)\n",
    "        first_actions = action_sequences[0]     # shape = (K, dim A)\n",
    "\n",
    "\n",
    "        for t in range(H):\n",
    "            actions = action_sequences[t]    # set of K actions, shape = (K, dim A)\n",
    "            scores += self.reward_model(states, actions)\n",
    "\n",
    "            s = torch.tensor(states).float()\n",
    "            a = torch.tensor(actions).float()\n",
    "\n",
    "            next_s = self.model(s, a)\n",
    "\n",
    "            # torch tensor to numpy array\n",
    "            # this cannot be skipped since a reward function takes numpy arrays as its inputs\n",
    "            states = next_s.detach().numpy()\n",
    "\n",
    "        best_seq = np.argmax(scores)\n",
    "\n",
    "        return action_sequences[0, best_seq]\n",
    "    \n",
    "    \n",
    "    \n",
    "    def solve_cross_entropy_opt(self, state, H, N, rho=0.2, tol=1e-2, max_it=100):\n",
    "        Ne = int(rho * N)\n",
    "        dimA = self.dimA\n",
    "        \n",
    "        self.model.eval()\n",
    "\n",
    "        states = np.tile(state, (N, 1)) # shape = (K, dim S)\n",
    "        scores = np.zeros(N)    # array which contains cumulative rewards of roll-outs\n",
    "        \n",
    "        \n",
    "        mu = np.zeros((H, 1, dimA))\n",
    "        sigma = (.5 * self.ctrl_range) * np.ones((H, 1, dimA))\n",
    "        \n",
    "        assert np.max(sigma) >= tol\n",
    "        best_action_so_far = None\n",
    "        best_score_so_far = -np.inf\n",
    "        trial = 0\n",
    "        while np.max(sigma) >= tol and trial < max_it:\n",
    "            action_sequences = truncnorm.rvs((-self.ctrl_range - mu) / sigma,\n",
    "                                             (self.ctrl_range - mu) / sigma,\n",
    "                                             loc=mu,\n",
    "                                             scale=sigma,\n",
    "                                             size=(H, N, dimA))\n",
    "            # generate K random action sequences of length H\n",
    "            for t in range(H):\n",
    "                actions = action_sequences[t]    # set of K actions, shape = (K, dim A)\n",
    "                scores += self.reward_model(states, actions)\n",
    "                s = torch.tensor(states).float()\n",
    "                a = torch.tensor(actions).float()\n",
    "                next_s = self.model(s, a)\n",
    "                # torch tensor to numpy array\n",
    "                states = next_s.detach().numpy()\n",
    "            # determine the elite samples of the group\n",
    "            indices = np.argsort(scores)[-Ne:]\n",
    "            idx = indices[-1]\n",
    "            if scores[idx] > best_score_so_far:\n",
    "                best_score_so_far = scores[idx]\n",
    "                best_action_so_far = action_sequences[0, idx, :]   \n",
    "            elite_samples = action_sequences[:, indices, :]\n",
    "            mu += self.alpha * (np.mean(elite_samples, axis=1, keepdims=True) - mu)\n",
    "            sigma += self.alpha * (np.std(elite_samples, axis=1, keepdims=True) - sigma)\n",
    "            trial += 1\n",
    "        if trial == max_it:\n",
    "            warnings.warn('maximum iteration exceeded', RuntimeWarning)\n",
    "        return best_action_so_far"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_rand_trajectories(env, transition_memory, num_trajectories):\n",
    "    print('collecting random trajectories...')\n",
    "    for i in range(num_trajectories):\n",
    "        # collect random trajectories\n",
    "        # able to be accelerated with mpi\n",
    "        state = env.reset()\n",
    "        for _ in range(200):\n",
    "            action = env.action_space.sample()\n",
    "            next_state, _, _, _ = env.step(action)\n",
    "            transition_memory.append(state, action, next_state)\n",
    "            state = next_state\n",
    "\n",
    "        if i % 10 == 9:\n",
    "            print('{} trajectories collected'.format(i + 1))\n",
    "    print('done')\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = MyPendulumEnv(g=10.0)\n",
    "state_dim = env.observation_space.shape[0]\n",
    "act_dim = env.action_space.shape[0]\n",
    "ctrl_range = env.action_space.high[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "collecting random trajectories...\n",
      "10 trajectories collected\n",
      "20 trajectories collected\n",
      "30 trajectories collected\n",
      "40 trajectories collected\n",
      "50 trajectories collected\n",
      "60 trajectories collected\n",
      "70 trajectories collected\n",
      "80 trajectories collected\n",
      "90 trajectories collected\n",
      "100 trajectories collected\n",
      "110 trajectories collected\n",
      "120 trajectories collected\n",
      "130 trajectories collected\n",
      "140 trajectories collected\n",
      "150 trajectories collected\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "agent = ModelBasedAgent(state_dim, act_dim, ctrl_range, pendulum_reward, hidden1=64, hidden2=64, mem_sz=30000)\n",
    "collect_rand_trajectories(env, agent.memory, num_trajectories=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "max_iter = 50\n",
    "num_ep_per_it = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pre-training...\n",
      "[iter 0] loss val : 0.1079\n",
      "[iter 1] loss val : 0.0621\n",
      "[iter 2] loss val : 0.0315\n",
      "[iter 3] loss val : 0.0386\n",
      "[iter 4] loss val : 0.0355\n",
      "[iter 5] loss val : 0.0315\n",
      "[iter 6] loss val : 0.0302\n",
      "[iter 7] loss val : 0.0324\n",
      "[iter 8] loss val : 0.0271\n",
      "[iter 9] loss val : 0.0201\n",
      "start MPC control...\n",
      "score (over 5 episodes) = -326.496346 ± 183.810657\n",
      "[iter 10] loss val : 0.0146\n",
      "score (over 5 episodes) = -394.098863 ± 119.354604\n",
      "[iter 11] loss val : 0.0249\n",
      "score (over 5 episodes) = -487.992963 ± 150.517995\n",
      "[iter 12] loss val : 0.0270\n",
      "score (over 5 episodes) = -337.978733 ± 119.109891\n",
      "[iter 13] loss val : 0.0189\n",
      "score (over 5 episodes) = -638.853377 ± 269.225570\n",
      "[iter 14] loss val : 0.0221\n",
      "score (over 5 episodes) = -634.344600 ± 242.629409\n",
      "[iter 15] loss val : 0.0198\n",
      "score (over 5 episodes) = -569.558449 ± 239.194871\n",
      "[iter 16] loss val : 0.0178\n",
      "score (over 5 episodes) = -430.670460 ± 174.526229\n",
      "[iter 17] loss val : 0.0163\n",
      "score (over 5 episodes) = -608.211137 ± 180.623511\n",
      "[iter 18] loss val : 0.0186\n",
      "score (over 5 episodes) = -624.213353 ± 131.120450\n",
      "[iter 19] loss val : 0.0176\n",
      "score (over 5 episodes) = -628.059709 ± 298.998774\n",
      "[iter 20] loss val : 0.0176\n",
      "score (over 5 episodes) = -292.713697 ± 94.567540\n",
      "[iter 21] loss val : 0.0191\n",
      "score (over 5 episodes) = -412.505956 ± 169.209175\n",
      "[iter 22] loss val : 0.0155\n",
      "score (over 5 episodes) = -533.225303 ± 176.012321\n",
      "[iter 23] loss val : 0.0111\n",
      "score (over 5 episodes) = -384.572264 ± 225.177732\n",
      "[iter 24] loss val : 0.0201\n",
      "score (over 5 episodes) = -628.837036 ± 244.885769\n",
      "[iter 25] loss val : 0.0111\n",
      "score (over 5 episodes) = -479.674254 ± 360.890355\n",
      "[iter 26] loss val : 0.0197\n",
      "score (over 5 episodes) = -246.196960 ± 154.195602\n",
      "[iter 27] loss val : 0.0141\n",
      "score (over 5 episodes) = -529.513516 ± 230.377283\n",
      "[iter 28] loss val : 0.0120\n",
      "score (over 5 episodes) = -529.064727 ± 383.864961\n",
      "[iter 29] loss val : 0.0061\n",
      "score (over 5 episodes) = -475.269150 ± 353.797883\n",
      "[iter 30] loss val : 0.0148\n",
      "score (over 5 episodes) = -355.765629 ± 221.753414\n",
      "[iter 31] loss val : 0.0093\n",
      "score (over 5 episodes) = -429.785406 ± 210.094381\n",
      "[iter 32] loss val : 0.0131\n",
      "score (over 5 episodes) = -406.347188 ± 259.120545\n",
      "[iter 33] loss val : 0.0082\n",
      "score (over 5 episodes) = -404.408453 ± 312.053615\n",
      "[iter 34] loss val : 0.0084\n",
      "score (over 5 episodes) = -454.588492 ± 210.279740\n",
      "[iter 35] loss val : 0.0087\n",
      "score (over 5 episodes) = -342.395354 ± 246.296753\n",
      "[iter 36] loss val : 0.0077\n",
      "score (over 5 episodes) = -343.761851 ± 117.706486\n",
      "[iter 37] loss val : 0.0083\n",
      "score (over 5 episodes) = -578.256261 ± 288.735533\n",
      "[iter 38] loss val : 0.0012\n",
      "score (over 5 episodes) = -292.187010 ± 93.726442\n",
      "[iter 39] loss val : 0.0074\n",
      "score (over 5 episodes) = -539.665937 ± 361.763216\n",
      "[iter 40] loss val : 0.0036\n",
      "score (over 5 episodes) = -354.968083 ± 271.951938\n",
      "[iter 41] loss val : 0.0050\n",
      "score (over 5 episodes) = -294.723035 ± 181.885666\n",
      "[iter 42] loss val : 0.0013\n",
      "score (over 5 episodes) = -246.772675 ± 148.306856\n",
      "[iter 43] loss val : 0.0087\n",
      "score (over 5 episodes) = -440.188823 ± 288.207259\n",
      "[iter 44] loss val : 0.0027\n",
      "score (over 5 episodes) = -241.553027 ± 2.002464\n",
      "[iter 45] loss val : 0.0062\n",
      "score (over 5 episodes) = -354.194150 ± 142.383908\n",
      "[iter 46] loss val : 0.0041\n",
      "score (over 5 episodes) = -241.976952 ± 2.859679\n",
      "[iter 47] loss val : 0.0026\n",
      "score (over 5 episodes) = -348.933822 ± 213.794352\n",
      "[iter 48] loss val : 0.0064\n",
      "score (over 5 episodes) = -301.652730 ± 192.480832\n",
      "[iter 49] loss val : 0.0026\n",
      "score (over 5 episodes) = -297.888857 ± 251.146235\n",
      "[iter 50] loss val : 0.0019\n",
      "score (over 5 episodes) = -426.901385 ± 238.226092\n",
      "[iter 51] loss val : 0.0051\n",
      "score (over 5 episodes) = -453.977167 ± 214.725196\n",
      "[iter 52] loss val : 0.0040\n",
      "score (over 5 episodes) = -530.942339 ± 363.748608\n",
      "[iter 53] loss val : 0.0024\n",
      "score (over 5 episodes) = -192.962303 ± 177.445353\n",
      "[iter 54] loss val : 0.0005\n",
      "score (over 5 episodes) = -341.101962 ± 245.907104\n",
      "[iter 55] loss val : 0.0009\n",
      "score (over 5 episodes) = -382.416198 ± 182.448591\n",
      "[iter 56] loss val : 0.0034\n",
      "score (over 5 episodes) = -385.252351 ± 326.831889\n",
      "[iter 57] loss val : 0.0003\n",
      "score (over 5 episodes) = -615.938404 ± 303.983987\n",
      "[iter 58] loss val : 0.0021\n",
      "score (over 5 episodes) = -499.051977 ± 273.165343\n",
      "[iter 59] loss val : 0.0013\n"
     ]
    }
   ],
   "source": [
    "# pre-train the network using randomly collected data\n",
    "num_epochs = 10\n",
    "epoch_size = len(agent.memory) // batch_size\n",
    "\n",
    "for i in range(max_iter + num_epochs):\n",
    "    # training loop\n",
    "    # train the model\n",
    "    if i == 0:\n",
    "        print('pre-training...')\n",
    "\n",
    "    if i == num_epochs:\n",
    "        print('start MPC control...')\n",
    "\n",
    "    if i < num_epochs:\n",
    "        # first train the network only using randomly collected data\n",
    "        for epoch in range(epoch_size):\n",
    "            loss = agent.train(batch_size=batch_size)\n",
    "        print('[iter {}] loss val : {:.4f}'.format(i, loss))\n",
    "\n",
    "    else:\n",
    "        scores = np.zeros(num_ep_per_it)\n",
    "        \n",
    "        for ep in range(num_ep_per_it):    \n",
    "            state = env.reset()\n",
    "            score = 0.\n",
    "            for _ in range(200):\n",
    "                if i % 5 == 0:\n",
    "                    pass\n",
    "                    # env.render()\n",
    "                # environment roll-out\n",
    "                # at each step, select an action using MPC (on-policy data)\n",
    "                action = agent.act(state, H=60, N=400, cross_entropy=False)\n",
    "                next_state, rew, _, _ = env.step(action)\n",
    "                agent.memory.append(state, action, next_state)\n",
    "                score += rew\n",
    "                state = next_state\n",
    "            scores[ep] = score\n",
    "        avg = np.mean(scores)\n",
    "        std = np.std(scores)\n",
    "        env.close()\n",
    "        print('score (over {} episodes) = {:4f}'.format(num_ep_per_it, avg), u'\\u00B1', '{:4f}'.format(std))\n",
    "        for _ in range(num_ep_per_it):\n",
    "            loss = agent.train(batch_size=batch_size)\n",
    "        print('[iter {}] loss val : {:.4f}'.format(i, loss), end='\\n')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
