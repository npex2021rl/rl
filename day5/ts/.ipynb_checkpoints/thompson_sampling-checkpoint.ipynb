{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thompson Sampling Practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from control import dare, ctrb\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from utils import value_iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Multi-armed Bandit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Finite Markov Decision Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RiverSwimEnv:\n",
    "    def __init__(self, num_states=6, success_prob=0.8):\n",
    "        self.num_states = num_states\n",
    "        self.num_actions = 2\n",
    "        self.success_prob = success_prob\n",
    "        self.state = None\n",
    "        \n",
    "        P = np.zeros((2 * num_states, num_states))\n",
    "        R = -np.ones((num_states, 2))\n",
    "        \n",
    "        for i in range(num_states):\n",
    "            P[i, max(0, i - 1)] = 1.\n",
    "            if i < num_states - 1:\n",
    "                P[num_states + i, i + 1] = success_prob\n",
    "                P[num_states + i, i] = 1. - success_prob\n",
    "            else:\n",
    "                P[num_states + i, i] = 1.\n",
    "        R[0, 0] = -0.8\n",
    "        R[num_states - 1, 1] = 0.\n",
    "        \n",
    "        self.R = R\n",
    "        self.P = P\n",
    "        \n",
    "    def reset(self):\n",
    "        self.state = 2\n",
    "        return self.state\n",
    "    \n",
    "    def step(self, action):\n",
    "        reward = self.R[self.state, action]\n",
    "        \n",
    "        if action == 0:\n",
    "            self.state = max(self.state - 1, 0)\n",
    "        elif action == 1:\n",
    "            if np.random.rand() < self.success_prob:\n",
    "                self.state = min(self.state + 1, self.num_states - 1)\n",
    "        \n",
    "        return self.state, reward\n",
    "    \n",
    "    @property\n",
    "    def R_true(self):\n",
    "        return self.R\n",
    "    \n",
    "    @property\n",
    "    def P_true(self):\n",
    "        return self.P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TSDEAgent:\n",
    "    def __init__(self, num_states, num_actions, alpha0, R):\n",
    "        self.num_states = num_states\n",
    "        self.num_actions = num_actions\n",
    "        self.alpha = alpha0    # parameters for Dirichlet priors & posteriors\n",
    "        self.R = R\n",
    "        self.policy = None\n",
    "        \n",
    "    def sample_model(self):\n",
    "        P = self.infer_model()\n",
    "        self.policy = value_iteration(P=P, R=self.R)[1]\n",
    "        \n",
    "    def act(self, state):\n",
    "        return self.policy[state]\n",
    "    \n",
    "    def update(self, state, action, next_state):\n",
    "        self.alpha[self.num_states * action + state, next_state] += 1\n",
    "        \n",
    "    def infer_model(self):\n",
    "        nrows = self.num_states * self.num_actions\n",
    "        P = np.zeros((nrows, self.num_states))\n",
    "        for i in range(nrows):\n",
    "            P[i] = np.random.dirichlet(self.alpha[i])\n",
    "        return P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TSDE(env, max_t=10000, verbose=True):\n",
    "    num_states = env.num_states\n",
    "    num_actions = env.num_actions\n",
    "    alpha0 = .1 * np.ones((num_states * num_actions, num_states))\n",
    "    agent = TSDEAgent(num_states=num_states, num_actions=num_actions, alpha0=alpha0, R=env.R_true)\n",
    "    t = 0\n",
    "    t_init = 0\n",
    "    ep_len = 1\n",
    "    cumul_reward = 0.\n",
    "    reward_arr = []\n",
    "    visit_count = np.zeros((num_states, num_actions), dtype=int)\n",
    "    state = env.reset()\n",
    "    while True:\n",
    "        agent.sample_model()\n",
    "        visit_count_init = visit_count\n",
    "        while t <= t_init + ep_len:\n",
    "            action = agent.act(state)\n",
    "            next_state, reward = env.step(action)\n",
    "            visit_count[state, action] += 1\n",
    "            \n",
    "            cumul_reward += reward\n",
    "            reward_arr.append(cumul_reward)\n",
    "            agent.update(state, action, next_state)\n",
    "            state = next_state\n",
    "            \n",
    "            if visit_count[state, action] > 2 * visit_count_init[state, action]:\n",
    "                break\n",
    "            \n",
    "            t += 1\n",
    "            if verbose and t % 5000 == 0:\n",
    "                print('{} steps completed'.format(t))\n",
    "            \n",
    "            if t >= max_t:\n",
    "                break\n",
    "        if t >= max_t:\n",
    "            break\n",
    "        ep_len = t - t_init\n",
    "        t_init = t\n",
    "        \n",
    "    return agent, reward_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transition probability : [[1.  0.  0.  0.  0.  0.  0.  0.  0.  0. ]\n",
      " [1.  0.  0.  0.  0.  0.  0.  0.  0.  0. ]\n",
      " [0.  1.  0.  0.  0.  0.  0.  0.  0.  0. ]\n",
      " [0.  0.  1.  0.  0.  0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  1.  0.  0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  1.  0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0.  1.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0.  0.  1.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0.  0.  0.  1.  0.  0. ]\n",
      " [0.  0.  0.  0.  0.  0.  0.  0.  1.  0. ]\n",
      " [0.7 0.3 0.  0.  0.  0.  0.  0.  0.  0. ]\n",
      " [0.  0.7 0.3 0.  0.  0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.7 0.3 0.  0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.7 0.3 0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0.7 0.3 0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0.  0.7 0.3 0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0.  0.  0.7 0.3 0.  0. ]\n",
      " [0.  0.  0.  0.  0.  0.  0.  0.7 0.3 0. ]\n",
      " [0.  0.  0.  0.  0.  0.  0.  0.  0.7 0.3]\n",
      " [0.  0.  0.  0.  0.  0.  0.  0.  0.  1. ]]\n",
      "reward : [[-0.8 -1. ]\n",
      " [-1.  -1. ]\n",
      " [-1.  -1. ]\n",
      " [-1.  -1. ]\n",
      " [-1.  -1. ]\n",
      " [-1.  -1. ]\n",
      " [-1.  -1. ]\n",
      " [-1.  -1. ]\n",
      " [-1.  -1. ]\n",
      " [-1.   0. ]]\n"
     ]
    }
   ],
   "source": [
    "env = RiverSwimEnv(num_states=10, success_prob=0.3)\n",
    "print('transition probability :', env.P_true)\n",
    "print('reward :', env.R_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000 steps completed\n",
      "10000 steps completed\n",
      "15000 steps completed\n",
      "20000 steps completed\n",
      "25000 steps completed\n",
      "30000 steps completed\n",
      "35000 steps completed\n",
      "40000 steps completed\n",
      "45000 steps completed\n",
      "50000 steps completed\n",
      "55000 steps completed\n",
      "60000 steps completed\n",
      "65000 steps completed\n",
      "70000 steps completed\n",
      "75000 steps completed\n",
      "80000 steps completed\n",
      "85000 steps completed\n",
      "90000 steps completed\n",
      "95000 steps completed\n",
      "100000 steps completed\n",
      "105000 steps completed\n",
      "110000 steps completed\n",
      "115000 steps completed\n",
      "120000 steps completed\n",
      "125000 steps completed\n",
      "130000 steps completed\n",
      "135000 steps completed\n",
      "140000 steps completed\n",
      "145000 steps completed\n",
      "150000 steps completed\n",
      "155000 steps completed\n",
      "160000 steps completed\n",
      "165000 steps completed\n",
      "170000 steps completed\n",
      "175000 steps completed\n",
      "180000 steps completed\n",
      "185000 steps completed\n",
      "190000 steps completed\n",
      "195000 steps completed\n",
      "200000 steps completed\n"
     ]
    }
   ],
   "source": [
    "T = 200000\n",
    "\n",
    "agent, reward_arr = TSDE(env=env, max_t=T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal policy : [1 1 1 1 1 1 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "gain, pi_opt = value_iteration(env.P_true, env.R_true)\n",
    "print('optimal policy :', pi_opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 23/50 [00:44<00:48,  1.78s/it]"
     ]
    }
   ],
   "source": [
    "num_trials = 50\n",
    "T = 50000\n",
    "\n",
    "regrets = np.zeros((num_trials, T))\n",
    "\n",
    "for i in tqdm(range(num_trials)):\n",
    "    reward_arr = TSDE(env=env, max_t=T, verbose=False)[1]\n",
    "    regrets[i] = gain * np.arange(1, T + 1) - np.array(reward_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_regret = np.mean(regrets, axis=0)\n",
    "std_regret = np.std(regrets, axis=0)\n",
    "\n",
    "xscale = -4\n",
    "unit_sz = 10 ** xscale\n",
    "x = unit_sz * np.arange(T)\n",
    "\n",
    "plt.plot(x, mean_regret, color='tab:red')\n",
    "plt.fill_between(x, mean_regret - .5 * std_regret, mean_regret + .5 * std_regret, alpha=0.2, color='tab:red')\n",
    "plt.xlabel(r'$T (\\times 10^{})$'.format(-xscale), fontsize=20)\n",
    "plt.ylabel(r'Regret$(T)$', fontsize=16)\n",
    "plt.title('Bayes Regret : Finite MDP', fontsize=20)\n",
    "plt.xlim(0, unit_sz * T)\n",
    "plt.ylim(0)\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Linear Quadratic Control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gain(theta, Q, R):\n",
    "    # Compute the gain matrix G of LQ system (A, B, Q, R).\n",
    "    n = Q.shape[0]\n",
    "    m = R.shape[0]\n",
    "    assert theta.shape == (n + m, n)\n",
    "    A = theta[:n].T\n",
    "    B = theta[n:].T\n",
    "    G = -dare(A, B, Q, R)[2]\n",
    "    return np.asarray(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearSystem:\n",
    "    def __init__(self, A, B, x0=None):\n",
    "        self.A = A\n",
    "        self.B = B\n",
    "        self.n = B.shape[0]\n",
    "        self.m = B.shape[1]\n",
    "        \n",
    "        if x0 is None:\n",
    "            self.x0 = np.zeros(self.n)\n",
    "        else:\n",
    "            self.x0 = x0\n",
    "        \n",
    "    def reset(self):\n",
    "        self.x = np.copy(self.x0)\n",
    "        return np.copy(self.x)\n",
    "    \n",
    "    def step(self, u):\n",
    "        # x_t+1 = A x_t + B u_t + w_t, w_t ~ N(0, I_n)\n",
    "        self.x = self.A @ self.x + self.B @ u + np.random.randn(self.n)\n",
    "        return np.copy(self.x)\n",
    "    \n",
    "    def is_stable(self, G: np.ndarray, delta) -> bool:\n",
    "        # Determine if an input controller is \\delta-stable.\n",
    "        # assert delta < 1.\n",
    "        assert G.shape == (self.m, self.n)\n",
    "        norm = np.linalg.norm(self.A + self.B @ G, ord=2)\n",
    "        return True if norm <= delta else False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Distribution:\n",
    "    def __init__(self, system: LinearSystem, Q, R, mean_prior=None, cov_prior=None):\n",
    "        self.system = system\n",
    "        self.Q = Q\n",
    "        self.R = R\n",
    "        self.n = Q.shape[0]\n",
    "        self.m = R.shape[0]\n",
    "        \n",
    "        d = self.n + self.m\n",
    "        \n",
    "        # location & scale parameters\n",
    "        if mean_prior is None:\n",
    "            self.mean = np.zeros((d, self.n))\n",
    "        else:\n",
    "            self.mean = mean_prior\n",
    "        if cov_prior is None:\n",
    "            self.cov = np.eye(d)\n",
    "        else:\n",
    "            self.cov = cov_prior\n",
    "\n",
    "        \n",
    "    def sample(self, delta):\n",
    "        L = np.linalg.cholesky(self.cov)\n",
    "        d, n = self.mean.shape\n",
    "        # rejection sampling\n",
    "        accept = False\n",
    "        while not accept:\n",
    "            theta = self.mean + L @ np.random.randn(d, n)\n",
    "            G = compute_gain(theta, self.Q, self.R)\n",
    "            accept = self.system.is_stable(G, delta)\n",
    "        return theta\n",
    "    \n",
    "    def update(self, z, x_next):\n",
    "        # Bayesian update of parameters\n",
    "        # z = (x_1, ..., x_n, u_1, ..., u_m)\n",
    "        cov_times_z = self.cov @ z\n",
    "        denom = 1 + np.dot(z, cov_times_z)\n",
    "        self.mean += np.outer(cov_times_z, x_next - z @ self.mean) / denom\n",
    "        self.cov -= np.outer(cov_times_z, cov_times_z) / denom\n",
    "        return\n",
    "    \n",
    "    @property\n",
    "    def cov_sz(self):\n",
    "        return np.prod(np.linalg.eigvals(self.cov))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PSRL_LQ(system: LinearSystem, Q, R, mean_prior=None, cov_prior=None, delta=0.999, max_t=10000, verbose=True):\n",
    "    distribution = Distribution(system, Q=Q, R=R, mean_prior=mean_prior, cov_prior=cov_prior)\n",
    "    \n",
    "    cost_arr = []\n",
    "    state_arr = []\n",
    "    cost = 0.\n",
    "    t = 0\n",
    "    t_init = 0\n",
    "    ep_len = 1\n",
    "    x = system.reset()\n",
    "    while True:\n",
    "        theta = distribution.sample(delta=delta)\n",
    "        G = compute_gain(theta, Q, R)\n",
    "        init_cov_sz = distribution.cov_sz\n",
    "        current_cov_sz = init_cov_sz\n",
    "        while (t <= t_init + ep_len) and (current_cov_sz >= 0.5 * init_cov_sz):\n",
    "            \n",
    "            u = G @ x\n",
    "            state_arr.append(x)\n",
    "            cost += np.dot(x, Q @ x) + np.dot(u, R @ u)\n",
    "            cost_arr.append(cost)\n",
    "            z = np.concatenate([x, u])\n",
    "            x_next = system.step(u)\n",
    "            distribution.update(z=z, x_next=x_next)\n",
    "            x = x_next\n",
    "            t += 1\n",
    "            if verbose and t % 1000 == 0:\n",
    "                print('{} steps completed'.format(t))\n",
    "            \n",
    "            if t >= max_t:\n",
    "                break\n",
    "        if t >= max_t:\n",
    "            break\n",
    "        ep_len = t - t_init\n",
    "        t_init = t\n",
    "        \n",
    "    return theta, cost_arr, state_arr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n, m = 3, 2\n",
    "\n",
    "h = 0.1\n",
    "A2 = np.array([\n",
    "    [1., 0., -h ** 2],\n",
    "    [0., 1., h ** 2],\n",
    "    [0., 0., 1.]\n",
    "])\n",
    "\n",
    "B2 = np.array([\n",
    "    [h / np.sqrt(2.), 0.],\n",
    "    [h / np.sqrt(2.), 0.],\n",
    "    [0., 1.]\n",
    "])\n",
    "x0 = np.array([0., 200., 0.])\n",
    "\n",
    "# check controllability\n",
    "C = ctrb(A2, B2)\n",
    "assert np.linalg.matrix_rank(C) == 3\n",
    "\n",
    "Q2 = np.diag([1., 1., .5])\n",
    "R2 = 0.1 * np.eye(2)\n",
    "\n",
    "theta2 = np.hstack([A2, B2]).T\n",
    "# set mean prior\n",
    "# encoding domain knowledge is possible!\n",
    "mu0 = theta2 + 0.3 * np.random.rand()\n",
    "mu0 = 0.5 * np.ones((n + m, n))\n",
    "# sigma0 = .01 * np.eye(n + m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "car_system = LinearSystem(A=A2, B=B2, x0=x0)\n",
    "optimal_cost = np.trace(dare(A2, B2, Q2, R2)[0])\n",
    "\n",
    "T = 2000\n",
    "\n",
    "theta, c_arr, s_arr = PSRL_LQ(car_system, Q=Q2, R=R2, mean_prior=mu0, delta=1.5, max_t=T)\n",
    "regrets = np.array(c_arr) - optimal_cost * np.arange(1, T + 1)\n",
    "thetas = theta\n",
    "states = np.array(s_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_ref = np.vstack([h * np.arange(T), h * np.arange(T), (np.pi / 4.) * np.ones(T)]).T\n",
    "x = states + x_ref\n",
    "x1 = x[:, 0]\n",
    "x2 = x[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x0[0], x0[1], marker='x', label='starting point', color='black')\n",
    "plt.plot(x1, x2, color='gray', label='vehicle trajectory')\n",
    "plt.plot(x_ref[:, 0], x_ref[:, 1], linestyle='dashed', color='black', label='reference path')\n",
    "plt.legend()\n",
    "plt.xlabel(r'$x$')\n",
    "plt.ylabel(r'$y$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's check how the regret goes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_trials = 50\n",
    "T = 2000\n",
    "\n",
    "regrets = np.zeros((num_trials, T))\n",
    "\n",
    "for i in tqdm(range(num_trials)):\n",
    "    c_arr = PSRL_LQ(car_system, Q=Q2, R=R2, mean_prior=mu0, delta=1.5, max_t=T, verbose=False)[1]\n",
    "    regrets[i] = np.array(c_arr) - optimal_cost * np.arange(1, T + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_regret = np.mean(regrets, axis=0)\n",
    "std_regret = np.std(regrets, axis=0)\n",
    "\n",
    "xscale = -3\n",
    "unit_sz = 10 ** xscale\n",
    "x = unit_sz * np.arange(T)\n",
    "\n",
    "plt.plot(x, mean_regret)\n",
    "plt.fill_between(x, mean_regret - .5 * std_regret, mean_regret + .5 * std_regret, alpha=0.2)\n",
    "plt.xlabel(r'$T (\\times 10^{})$'.format(-scale), fontsize=20)\n",
    "plt.ylabel(r'Regret$(T)$', fontsize=16)\n",
    "plt.title('Bayes Regret : LQ', fontsize=20)\n",
    "plt.xlim(0, unit_sz * T)\n",
    "plt.ylim(0)\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
