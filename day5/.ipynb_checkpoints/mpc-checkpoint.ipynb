{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import truncnorm\n",
    "import warnings\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from torch.nn import MSELoss\n",
    "from my_pendulum import MyPendulumEnv\n",
    "from reward_ftns import pendulum_reward\n",
    "from memory import TransitionMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransitionModel(nn.Module):\n",
    "    def __init__(self, state_dim, act_dim, hidden1, hidden2):\n",
    "        super(TransitionModel, self).__init__()\n",
    "        self.state_dim = state_dim\n",
    "        self.act_dim = act_dim\n",
    "        self.fc1 = nn.Linear(state_dim + act_dim, hidden1)\n",
    "        self.fc2 = nn.Linear(hidden1, hidden2)\n",
    "        self.fc3 = nn.Linear(hidden2, state_dim)\n",
    "\n",
    "    def forward(self, state, act):\n",
    "        x = torch.cat([state, act], dim=1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        delta = self.fc3(x)\n",
    "\n",
    "        next_state = state + delta  # \\hat{s}_{t+1} = s_t + f(s_t, a_t ; \\theta)\n",
    "        \n",
    "        return next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelBasedAgent:\n",
    "    def __init__(self,\n",
    "                 state_dim,\n",
    "                 act_dim,\n",
    "                 ctrl_range,\n",
    "                 hidden1=400,\n",
    "                 hidden2=400,\n",
    "                 lr=0.001,\n",
    "                 mem_sz=20000\n",
    "                 ):\n",
    "        self.dimS = state_dim\n",
    "        self.dimA = act_dim\n",
    "        self.ctrl_range = ctrl_range\n",
    "        self.model = TransitionModel(state_dim, act_dim, hidden1, hidden2)\n",
    "        self.memory = TransitionMemory(state_dim, act_dim, maxlen=mem_sz)\n",
    "\n",
    "        self.optimizer = Adam(self.model.parameters(), lr=lr)\n",
    "        \n",
    "        self.alpha = 0.5    # smoothing parameter of cross-entropy optimization\n",
    "        self.Ne = 20        # number of elite samples\n",
    "\n",
    "    def train(self, batch_size):\n",
    "        self.model.train()\n",
    "        # note that training of the dynamics does not depend on any reward info\n",
    "        (state_batch, act_batch, next_state_batch) = self.memory.sample_batch(batch_size)\n",
    "\n",
    "        state_batch = torch.tensor(state_batch).float()\n",
    "        act_batch = torch.tensor(act_batch).float()\n",
    "        next_state_batch = torch.tensor(next_state_batch).float()\n",
    "\n",
    "        prediction = self.model(state_batch, act_batch)\n",
    "\n",
    "\n",
    "        loss_ftn = MSELoss()\n",
    "        loss = loss_ftn(prediction, next_state_batch)\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        loss_val = loss.detach().numpy()\n",
    "\n",
    "        return loss_val\n",
    "\n",
    "    def execute_action(self, state, rew_ftn, K, H):\n",
    "        \"\"\"\n",
    "        # generate K trajectories using the model of dynamics and random action sampling, and perform MPC\n",
    "        # Remark! K roll-outs can be done simultaneously!\n",
    "\n",
    "        given a state, execute an action based on random-sampling shooting method\n",
    "        :param state: current state(numpy array)\n",
    "        :param rew_ftn: vectorized reward function\n",
    "        :param K: number of candidate action sequences to generate\n",
    "        :param H: length of time horizon\n",
    "        :return: action to be executed(numpy array)\n",
    "        \"\"\"\n",
    "        assert K > 0 and H > 0\n",
    "\n",
    "        dimA = self.dimA\n",
    "\n",
    "        self.model.eval()\n",
    "\n",
    "        states = np.tile(state, (K, 1)) # shape = (K, dim S)\n",
    "        scores = np.zeros(K)    # array which contains cumulative rewards of roll-outs\n",
    "\n",
    "        # generate K random action sequences of length H\n",
    "        action_sequences = self.ctrl_range * (2. * np.random.rand(H, K, dimA) - 1.)\n",
    "        first_actions = action_sequences[0]     # shape = (K, dim A)\n",
    "\n",
    "\n",
    "        for t in range(H):\n",
    "            actions = action_sequences[t]    # set of K actions, shape = (K, dim A)\n",
    "            scores += rew_ftn(states, actions)\n",
    "\n",
    "            s = torch.tensor(states).float()\n",
    "            a = torch.tensor(actions).float()\n",
    "\n",
    "            next_s = self.model(s, a)\n",
    "\n",
    "            # torch tensor to numpy array\n",
    "            # this cannot be skipped since a reward function takes numpy arrays as its inputs\n",
    "            states = next_s.detach().numpy()\n",
    "\n",
    "        best_seq = np.argmax(scores)\n",
    "\n",
    "        return action_sequences[0, best_seq]\n",
    "    \n",
    "    \n",
    "    def act(self, state, rew_ftn, N, H, tol=1e-2, max_it=100):\n",
    "        dimA = self.dimA\n",
    "        \n",
    "        self.model.eval()\n",
    "\n",
    "        states = np.tile(state, (N, 1)) # shape = (K, dim S)\n",
    "        scores = np.zeros(N)    # array which contains cumulative rewards of roll-outs\n",
    "        \n",
    "        \n",
    "        mu = np.zeros((H, 1, dimA))\n",
    "        sigma = (.5 * self.ctrl_range) * np.ones((H, 1, dimA))\n",
    "        \n",
    "        trial = 0\n",
    "        while np.max(sigma) >= tol and trial < max_it:\n",
    "            action_sequences = truncnorm.rvs((-self.ctrl_range - mu) / sigma,\n",
    "                                             (self.ctrl_range - mu) / sigma,\n",
    "                                             loc=mu,\n",
    "                                             scale=sigma,\n",
    "                                             size=(H, N, dimA))\n",
    "            # generate K random action sequences of length H\n",
    "            for t in range(H):\n",
    "                actions = action_sequences[t]    # set of K actions, shape = (K, dim A)\n",
    "                scores += rew_ftn(states, actions)\n",
    "                s = torch.tensor(states).float()\n",
    "                a = torch.tensor(actions).float()\n",
    "\n",
    "                next_s = self.model(s, a)\n",
    "\n",
    "                # torch tensor to numpy array\n",
    "                # this cannot be skipped since a reward function takes numpy arrays as its inputs\n",
    "                states = next_s.detach().numpy()\n",
    "            indices = np.argsort(scores)[:self.Ne]\n",
    "            elite_samples = action_sequences[:, indices, :]\n",
    "            mu += self.alpha * (np.mean(elite_samples, axis=1, keepdims=True) - mu)\n",
    "            sigma += self.alpha * (np.std(elite_samples, axis=1, keepdims=True) - sigma)\n",
    "            trial += 1\n",
    "        if trial == max_it:\n",
    "            warnings.warn('maximum iteration exceeded', RuntimeWarning)\n",
    "        return np.clip(mu[0, 0, :], -self.ctrl_range, self.ctrl_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_rand_trajectories(env, transition_memory, num_trajectories):\n",
    "    print('collecting random trajectories...')\n",
    "    for i in range(num_trajectories):\n",
    "        # collect random trajectories\n",
    "        # able to be accelerated with mpi\n",
    "        state = env.reset()\n",
    "        for _ in range(200):\n",
    "            action = env.action_space.sample()\n",
    "            next_state, _, _, _ = env.step(action)\n",
    "            transition_memory.append(state, action, next_state)\n",
    "            state = next_state\n",
    "\n",
    "        if i % 10 == 9:\n",
    "            print('{} trajectories collected'.format(i + 1))\n",
    "    print('done')\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = MyPendulumEnv(g=10.0)\n",
    "state_dim = env.observation_space.shape[0]\n",
    "act_dim = env.action_space.shape[0]\n",
    "ctrl_range = env.action_space.high[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "collecting random trajectories...\n",
      "10 trajectories collected\n",
      "20 trajectories collected\n",
      "30 trajectories collected\n",
      "40 trajectories collected\n",
      "50 trajectories collected\n",
      "60 trajectories collected\n",
      "70 trajectories collected\n",
      "80 trajectories collected\n",
      "90 trajectories collected\n",
      "100 trajectories collected\n",
      "110 trajectories collected\n",
      "120 trajectories collected\n",
      "130 trajectories collected\n",
      "140 trajectories collected\n",
      "150 trajectories collected\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "agent = ModelBasedAgent(state_dim, act_dim, ctrl_range, hidden1=64, hidden2=64, mem_sz=30000)\n",
    "collect_rand_trajectories(env, agent.memory, num_trajectories=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "max_iter = 50\n",
    "num_ep_per_it = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pre-training...\n",
      "[iter 0] loss val : 0.0959\n",
      "[iter 1] loss val : 0.0630\n",
      "[iter 2] loss val : 0.0315\n",
      "[iter 3] loss val : 0.0297\n",
      "[iter 4] loss val : 0.0413\n",
      "[iter 5] loss val : 0.0335\n",
      "[iter 6] loss val : 0.0261\n",
      "[iter 7] loss val : 0.0267\n",
      "[iter 8] loss val : 0.0235\n",
      "[iter 9] loss val : 0.0275\n",
      "start MPC control...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sju5379/anaconda3/envs/drl/lib/python3.6/site-packages/ipykernel_launcher.py:126: RuntimeWarning: maximum iteration exceeded\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.00819646]\n",
      "[0.49836312]\n",
      "[-0.42149162]\n",
      "[0.15803429]\n",
      "[0.6770624]\n",
      "[0.39808148]\n",
      "[-0.05485569]\n",
      "[0.283974]\n",
      "[0.0540125]\n",
      "[-0.68555005]\n"
     ]
    }
   ],
   "source": [
    "# pre-train the network using randomly collected data\n",
    "num_epochs = 10\n",
    "epoch_size = len(agent.memory) // batch_size\n",
    "\n",
    "for i in range(max_iter + num_epochs):\n",
    "    # training loop\n",
    "    # train the model\n",
    "    if i == 0:\n",
    "        print('pre-training...')\n",
    "\n",
    "    if i == num_epochs:\n",
    "        print('start MPC control...')\n",
    "\n",
    "    if i < num_epochs:\n",
    "        # first train the network only using randomly collected data\n",
    "        for epoch in range(epoch_size):\n",
    "            loss = agent.train(batch_size=batch_size)\n",
    "        print('[iter {}] loss val : {:.4f}'.format(i, loss))\n",
    "\n",
    "    else:\n",
    "        scores = np.zeros(num_ep_per_it)\n",
    "        \n",
    "        for ep in range(num_ep_per_it):    \n",
    "            state = env.reset()\n",
    "            score = 0.\n",
    "            for _ in range(200):\n",
    "                if i % 5 == 0:\n",
    "                    pass\n",
    "                    # env.render()\n",
    "                # environment roll-out\n",
    "                # at each step, select an action using MPC (on-policy data)\n",
    "                action = agent.act(state, pendulum_reward, N=100, H=50)\n",
    "                print(action)\n",
    "                next_state, rew, _, _ = env.step(action)\n",
    "                agent.memory.append(state, action, next_state)\n",
    "                score += rew\n",
    "                state = next_state\n",
    "            scores[ep] = score\n",
    "        avg = np.mean(scores)\n",
    "        std = np.std(scores)\n",
    "        env.close()\n",
    "        print('score (over {} episodes) = {:4f}'.format(num_ep_per_it, avg), u'\\u00B1', '{:4f}'.format(std))\n",
    "        for _ in range(num_ep_per_it):\n",
    "            loss = agent.train(batch_size=batch_size)\n",
    "        print('[iter {}] loss val : {:.4f}'.format(i, loss), end='\\n')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
